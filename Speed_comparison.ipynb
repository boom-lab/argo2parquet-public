{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ffcc73d-56f7-41e3-9d61-e01ab4d8f761",
   "metadata": {},
   "source": [
    "# Read test - Map of Argo and GLODAP temperature measurements in the South Eastern Indian Ocean (Poseidon version)\n",
    "\n",
    "This notebook tests some basic filtering for a couple of different partitionings of the Argo Core parquet database, namely:\n",
    "* reorganizing the dataframes so that each takes up around 100 MB (min. recommended by dask);\n",
    "* reorganizing the dataframes so that each takes up around 300 MB (max. recommended by dask);\n",
    "* saving to disk so that data is split by year-month-day, as users will most likely be interested in a specific time range.\n",
    "\n",
    "The data are stored across multiple files: we will load into memory only what we need by applying some filters, and we will create a map showing the temperature measurements in the North West Atlantic.\n",
    "\n",
    "##### Note on Poseidon\n",
    "\n",
    "In this example we will access data stored in WHOI's **Poseidon cluster**. Reading data from WHOI's Amazon S3 data lake is slightly different and we refer you to dedicated examples (manipulating the data once loaded into the memory does not change).\n",
    "\n",
    "NB: to use this example you need to have access to WHOI's VPN or network, **and** to Boom lab's shared storage at `/vortexfs1/share/boom`. The notebook should also be executed from Poseidon.\n",
    "\n",
    "#### Getting started\n",
    "\n",
    "We first load all the modules we need, and define the geographical coordinates that the limit the area that we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d0f1b33-cc9b-4b38-9fc1-7880c4083901",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import xarray as xr\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "\n",
    "# Paths on Poseidon cluster\n",
    "pqt_dir = '/vortexfs1/share/boom/data/nc2pqt_test/pqt2/'\n",
    "\n",
    "pqt_100 = pqt_dir + 'partition100MB/'\n",
    "pqt_300 = pqt_dir + 'partition300MB/'\n",
    "pqt_juld = pqt_dir + 'partitionYYYYMM/'\n",
    "\n",
    "lat0 = 34\n",
    "lat1 = 80\n",
    "lon0 = -78\n",
    "lon1 = -50\n",
    "\n",
    "# pre-load schema\n",
    "schema_path = \"/vortexfs1/share/boom/data/nc2pqt_test/pqt/data/metadata/ArgoPHY_schema.metadata\"\n",
    "PHY_schema = pq.read_schema(schema_path)\n",
    "todrop = [\"DOXY\",\"DOXY_ADJUSTED\",\"DOXY_ADJUSTED_QC\",\"DOXY_ADJUSTED_ERROR\",\"DOXY_QC\"]\n",
    "for name in todrop:\n",
    "    idx = PHY_schema.get_field_index(name)\n",
    "    PHY_schema = PHY_schema.remove(idx)\n",
    "\n",
    "PHY_schema = PHY_schema.append(\n",
    "    pa.field('JULD_D', \n",
    "             pa.from_numpy_dtype(np.dtype('datetime64[ns]'))\n",
    "            )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62e3698-5cae-4823-a902-d2f81d824b50",
   "metadata": {},
   "source": [
    "## Timing tests\n",
    "\n",
    "The geographical coordinates are stored in the variables 'LATITUDE'and 'LONGITUDE'. We then generate the filter, with its syntax being: `[[(column, op, val), …],…]` where `column` is the variable name, and `val` is the value to for the operator `op`, which accepts `[==, =, >, >=, <, <=, !=, in, not in]`. Similarly, we will also filter by depth through the pressure values in 'PRES_ADJUSTED', to restrain our selection to the first 50m of the ocean.\n",
    "\n",
    "Let's set up the filters first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c039d9e-b084-4f6b-b723-9f2a6fa77c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "time0 = datetime.utcnow() - timedelta(days=365)\n",
    "time1 = time0 + timedelta(days=90)\n",
    "\n",
    "ref_var = 'TEMP_ADJUSTED'\n",
    "cols = [ref_var,\"LATITUDE\",\"LONGITUDE\",\"PRES_ADJUSTED\"]\n",
    "filter_to_apply = [(\"JULD\",\">=\",time0),(\"JULD\",\"<\",time1),\n",
    "                      (\"LATITUDE\",\">=\",lat0), (\"LATITUDE\",\"<=\",lat1),\n",
    "                      (\"LONGITUDE\",\">=\",lon0), (\"LONGITUDE\",\"<=\",lon1),\n",
    "                      (\"PRES_ADJUSTED\",\">=\",0),(\"PRES_ADJUSTED\",\"<=\",50),\n",
    "                      (ref_var,\">=\",-1e30),(ref_var,\"<=\",+1e30)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c8d7df-cb61-46ea-b90e-27ffa9700ea4",
   "metadata": {},
   "source": [
    "Now we time how long it takes to load the filtered data with each different partitioning scheme.\n",
    "\n",
    "### pyarrow only\n",
    "\n",
    "We start using only pyarrow. While dask will likely improve the performance, we first want to see how pyarrow performs. Note the that pyarrow is the same engine used by dask, and that it supports multi-threaded column reads natively and by default.\n",
    "\n",
    "#### 100 MB in-memory partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14b19a2b-60f6-4e94-8da4-479bec3050b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.8 s, sys: 2.67 s, total: 15.5 s\n",
      "Wall time: 3.73 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "argo_ds = pq.ParquetDataset(\n",
    "    pqt_100, \n",
    "    schema=PHY_schema,\n",
    "    filters=filter_to_apply\n",
    ")\n",
    "argo_df = argo_ds.read(columns=cols).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b947ae85-6e74-4ba2-9f4a-60cea137d03b",
   "metadata": {},
   "source": [
    "#### 300 MB in-memory partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ec2be78-b6c5-44a3-a278-37263aff4194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.1 s, sys: 2.67 s, total: 19.8 s\n",
      "Wall time: 1.93 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "argo_ds = pq.ParquetDataset(\n",
    "    pqt_300, \n",
    "    schema=PHY_schema,\n",
    "    filters=filter_to_apply\n",
    ")\n",
    "argo_df = argo_ds.read(columns=cols).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f31b229-aa72-400b-af5b-989fc1e443b2",
   "metadata": {},
   "source": [
    "#### YYYY-MM on-disk partitions (filtering on JULD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b777835b-ee8c-4696-acaf-6b2838013253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 2s, sys: 1min 18s, total: 9min 20s\n",
      "Wall time: 6min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "argo_ds = pq.ParquetDataset(\n",
    "    pqt_juld, \n",
    "    schema=PHY_schema,\n",
    "    filters=filter_to_apply\n",
    ")\n",
    "argo_df = argo_ds.read(columns=cols).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50f28a9b-7ed6-42ae-ba7d-74caf9e73a60",
   "metadata": {},
   "source": [
    "#### YYYY-MM on-disk partitions (filtering on partitioned parameter JULD_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ad2dc4a-0c02-4170-8496-df3a6ee40768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 48.3 s, sys: 25.2 s, total: 1min 13s\n",
      "Wall time: 2min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "filter_to_apply_D = [(\"JULD_D\",\">=\",time0),(\"JULD_D\",\"<\",time1),\n",
    "                      (\"LATITUDE\",\">=\",lat0), (\"LATITUDE\",\"<=\",lat1),\n",
    "                      (\"LONGITUDE\",\">=\",lon0), (\"LONGITUDE\",\"<=\",lon1),\n",
    "                      (\"PRES_ADJUSTED\",\">=\",0),(\"PRES_ADJUSTED\",\"<=\",50),\n",
    "                      (ref_var,\">=\",-1e30),(ref_var,\"<=\",+1e30)]\n",
    "argo_ds = pq.ParquetDataset(\n",
    "    pqt_juld, \n",
    "    schema=PHY_schema,\n",
    "    filters=filter_to_apply_D\n",
    ")\n",
    "argo_df = argo_ds.read(columns=cols).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b260c7-b6b0-4427-a00e-65a437b69b29",
   "metadata": {},
   "source": [
    "### pyarrow+dask\n",
    "\n",
    "We start using only pyarrow. While dask will likely improve the performance, we first want to see how pyarrow performs. Note the that pyarrow is the same engine used by dask, and that it supports multi-threaded column reads natively and by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dcfde179-6534-4fcf-bca9-59aeb9cc3e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb80babd-a3d4-4895-8d18-8855fd0fb6dd",
   "metadata": {},
   "source": [
    "#### 100 MB in-memory partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68459eba-de40-4e59-aa8b-9947a77ab482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.06 s, sys: 3.42 s, total: 12.5 s\n",
      "Wall time: 2.4 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEMP_ADJUSTED</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>PRES_ADJUSTED</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=287</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>float32</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<div>Dask Name: read_parquet, 1 expression</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                TEMP_ADJUSTED LATITUDE LONGITUDE PRES_ADJUSTED\n",
       "npartitions=287                                               \n",
       "                      float32  float64   float64       float32\n",
       "                          ...      ...       ...           ...\n",
       "...                       ...      ...       ...           ...\n",
       "                          ...      ...       ...           ...\n",
       "                          ...      ...       ...           ...\n",
       "Dask Name: read_parquet, 1 expression\n",
       "Expr=FromGraph(bed8962)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ddf = dd.read_parquet(\n",
    "    pqt_100,\n",
    "    engine=\"pyarrow\",\n",
    "    storage_options={\"anon\": True, \"use_ssl\": True} ,\n",
    "    schema=PHY_schema,\n",
    "    columns = cols,\n",
    "    filters = filter_to_apply\n",
    "    )\n",
    "ddf.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15fd028-d418-4460-b45d-ddb556dc0580",
   "metadata": {},
   "source": [
    "#### 300 MB in-memory partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d75f168e-cccd-4245-80f9-8cb1f6d02cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.1 s, sys: 6.06 s, total: 18.1 s\n",
      "Wall time: 1.82 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><strong>Dask DataFrame Structure:</strong></div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEMP_ADJUSTED</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>PRES_ADJUSTED</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npartitions=184</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>float32</td>\n",
       "      <td>float64</td>\n",
       "      <td>float64</td>\n",
       "      <td>float32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<div>Dask Name: read_parquet, 1 expression</div>"
      ],
      "text/plain": [
       "Dask DataFrame Structure:\n",
       "                TEMP_ADJUSTED LATITUDE LONGITUDE PRES_ADJUSTED\n",
       "npartitions=184                                               \n",
       "                      float32  float64   float64       float32\n",
       "                          ...      ...       ...           ...\n",
       "...                       ...      ...       ...           ...\n",
       "                          ...      ...       ...           ...\n",
       "                          ...      ...       ...           ...\n",
       "Dask Name: read_parquet, 1 expression\n",
       "Expr=FromGraph(bda53fc)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "ddf = dd.read_parquet(\n",
    "    pqt_300,\n",
    "    engine=\"pyarrow\",\n",
    "    storage_options={\"anon\": True, \"use_ssl\": True},\n",
    "    columns = cols,\n",
    "    filters = filter_to_apply\n",
    "    )\n",
    "ddf.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8231fe13-1fbf-4488-a2bd-f58bc3a064a3",
   "metadata": {},
   "source": [
    "#### YYYY-MM on-disk partitions (JULD_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee7e133-cbb3-4313-9062-7b789c3c3e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ddf = dd.read_parquet(\n",
    "    pqt_juld,\n",
    "    engine=\"pyarrow\",\n",
    "    storage_options={\"anon\": True, \"use_ssl\": True},\n",
    "    columns = cols,\n",
    "    filters = filter_to_apply_D\n",
    "    )\n",
    "ddf.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ac5d72-30b9-4933-bd8d-6002271fc6a9",
   "metadata": {},
   "source": [
    "### pyarrow+dask cluster\n",
    "\n",
    "We start using only pyarrow. While dask will likely improve the performance, we first want to see how pyarrow performs. Note the that pyarrow is the same engine used by dask, and that it supports multi-threaded column reads natively and by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134456fe-1f12-43ef-b763-7bf30926f10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "client = Client(n_workers=10, threads_per_worker=10, processes=True, memory_limit='auto')\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c4f6d9-5e3f-42b5-9013-fc7c776a0c75",
   "metadata": {},
   "source": [
    "#### 100 MB in-memory partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4e972f-43c8-4de7-86f3-d03a669f5ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ddf = dd.read_parquet(\n",
    "    pqt_100,\n",
    "    engine=\"pyarrow\",\n",
    "    storage_options={\"anon\": True, \"use_ssl\": True} ,\n",
    "    schema=PHY_schema,\n",
    "    columns = cols,\n",
    "    filters = filter_to_apply\n",
    "    )\n",
    "ddf.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bb9361-580a-4452-990e-58753bab0cff",
   "metadata": {},
   "source": [
    "#### 300 MB in-memory partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9be50d-bfe6-4715-96e8-096109300b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ddf = dd.read_parquet(\n",
    "    pqt_300,\n",
    "    engine=\"pyarrow\",\n",
    "    storage_options={\"anon\": True, \"use_ssl\": True},\n",
    "    columns = cols,\n",
    "    filters = filter_to_apply\n",
    "    )\n",
    "ddf.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef51ec0-ed2d-4f2d-b9be-21b79205f1ed",
   "metadata": {},
   "source": [
    "#### YYYY-MM on-disk partitions (on JULD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139564c0-e944-49f7-b70e-e62f709fd5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ddf = dd.read_parquet(\n",
    "    pqt_juld,\n",
    "    engine=\"pyarrow\",\n",
    "    storage_options={\"anon\": True, \"use_ssl\": True},\n",
    "    columns = cols,\n",
    "    filters = filter_to_apply\n",
    "    )\n",
    "ddf.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fc0898-836c-4db9-a59f-46c04268addd",
   "metadata": {},
   "source": [
    "#### YYYY-MM on-disk partitions (on JULD_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a4a70f-3557-46bc-8b03-4b868ae3bc71",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ddf = dd.read_parquet(\n",
    "    pqt_juld,\n",
    "    engine=\"pyarrow\",\n",
    "    storage_options={\"anon\": True, \"use_ssl\": True},\n",
    "    columns = cols,\n",
    "    filters = filter_to_apply_D\n",
    "    )\n",
    "ddf.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcf7faa-c470-4171-ba94-f44f074cf67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
