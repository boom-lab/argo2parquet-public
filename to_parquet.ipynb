{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import gsw\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import pyarrow.parquet as pq\n",
    "import requests as rq\n",
    "import os\n",
    "from urllib.parse import urljoin, urlencode\n",
    "import argo_tools as at\n",
    "import argopy\n",
    "from argopy import DataFetcher as ArgoDataFetcher\n",
    "local_gdac = '/vortexfs1/share/boom/data/gdac_snapshot/202403-ArgoData'\n",
    "#Path.mkdir('/vortexfs1/share/boom/projects/n2o/pq')\n",
    "outdir_nc = '/vortexfs1/share/boom/data/nc2pqt_test/GDAC/'\n",
    "outdir_pqt = '/vortexfs1/share/boom/data/nc2pqt_test/PQT/' #'/vortexfs1/share/boom/projects/n2o/pq'\n",
    "# argopy.set_options(mode='expert',src='gdac',ftp=local_gdac)\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "import glob\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vortexfs1/home/enrico.milanese/projects/ARGO/nc2parquet/argo_tools.py:50: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "  gdac_index = pd.read_csv(gdac_path,delimiter=',',header=8,parse_dates=['date','date_update'],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>> Destination file: /vortexfs1/share/boom/data/nc2pqt_test/GDAC/1901467_Sprof.nc.\n",
      ">>> File 1901467_Sprof.nc already exists. Leaving current version.\n",
      ">>>> Destination file: /vortexfs1/share/boom/data/nc2pqt_test/GDAC/1901468_Sprof.nc.\n",
      ">>> File 1901468_Sprof.nc already exists. Leaving current version.\n",
      ">>>> Destination file: /vortexfs1/share/boom/data/nc2pqt_test/GDAC/1901668_Sprof.nc.\n",
      ">>> File 1901668_Sprof.nc already exists. Leaving current version.\n",
      ">>>> Destination file: /vortexfs1/share/boom/data/nc2pqt_test/GDAC/4901283_Sprof.nc.\n",
      ">>> File 4901283_Sprof.nc already exists. Leaving current version.\n",
      ">>>> Destination file: /vortexfs1/share/boom/data/nc2pqt_test/GDAC/4901460_Sprof.nc.\n",
      ">>> File 4901460_Sprof.nc already exists. Leaving current version.\n",
      ">>>> Destination file: /vortexfs1/share/boom/data/nc2pqt_test/GDAC/6901174_Sprof.nc.\n",
      ">>> File 6901174_Sprof.nc already exists. Leaving current version.\n",
      ">>>> Destination file: /vortexfs1/share/boom/data/nc2pqt_test/GDAC/6901175_Sprof.nc.\n",
      ">>> File 6901175_Sprof.nc already exists. Leaving current version.\n",
      ">>>> Destination file: /vortexfs1/share/boom/data/nc2pqt_test/GDAC/1900650_Sprof.nc.\n",
      ">>> File 1900650_Sprof.nc already exists. Leaving current version.\n",
      ">>>> Destination file: /vortexfs1/share/boom/data/nc2pqt_test/GDAC/1900651_Sprof.nc.\n",
      ">>> File 1900651_Sprof.nc already exists. Leaving current version.\n",
      ">>>> Destination file: /vortexfs1/share/boom/data/nc2pqt_test/GDAC/1900652_Sprof.nc.\n",
      ">>> File 1900652_Sprof.nc already exists. Leaving current version.\n",
      ">>>> Destination file: /vortexfs1/share/boom/data/nc2pqt_test/GDAC/1900943_Sprof.nc.\n",
      ">>> File 1900943_Sprof.nc already exists. Leaving current version.\n",
      ">>>> Destination file: /vortexfs1/share/boom/data/nc2pqt_test/GDAC/1901360_Sprof.nc.\n",
      ">>> File 1901360_Sprof.nc already exists. Leaving current version.\n",
      ">>>> Destination file: /vortexfs1/share/boom/data/nc2pqt_test/GDAC/1901361_Sprof.nc.\n",
      ">>> File 1901361_Sprof.nc already exists. Leaving current version.\n",
      ">>>> Destination file: /vortexfs1/share/boom/data/nc2pqt_test/GDAC/1901363_Sprof.nc.\n",
      ">>> File 1901363_Sprof.nc already exists. Leaving current version.\n",
      ">>>> Destination file: /vortexfs1/share/boom/data/nc2pqt_test/GDAC/1901364_Sprof.nc.\n",
      ">>> File 1901364_Sprof.nc already exists. Leaving current version.\n",
      ">>>> Destination file: /vortexfs1/share/boom/data/nc2pqt_test/GDAC/1901365_Sprof.nc.\n",
      ">>> File 1901365_Sprof.nc already exists. Leaving current version.\n",
      ">>>> Destination file: /vortexfs1/share/boom/data/nc2pqt_test/GDAC/3900281_Sprof.nc.\n",
      ">>> File 3900281_Sprof.nc already exists. Leaving current version.\n",
      ">>>> Destination file: /vortexfs1/share/boom/data/nc2pqt_test/GDAC/3902122_Sprof.nc.\n",
      ">>> File 3902122_Sprof.nc already exists. Leaving current version.\n",
      ">>>> Destination file: /vortexfs1/share/boom/data/nc2pqt_test/GDAC/3902123_Sprof.nc.\n",
      ">>> File 3902123_Sprof.nc already exists. Leaving current version.\n",
      ">>>> Destination file: /vortexfs1/share/boom/data/nc2pqt_test/GDAC/6900524_Sprof.nc.\n",
      ">>> File 6900524_Sprof.nc already exists. Leaving current version.\n",
      ">>>> Destination file: /vortexfs1/share/boom/data/nc2pqt_test/GDAC/6900525_Sprof.nc.\n",
      ">>> File 6900525_Sprof.nc already exists. Leaving current version.\n",
      ">>>> Destination file: /vortexfs1/share/boom/data/nc2pqt_test/GDAC/6900628_Sprof.nc.\n",
      ">>> File 6900628_Sprof.nc already exists. Leaving current version.\n",
      ">>>> Destination file: /vortexfs1/share/boom/data/nc2pqt_test/GDAC/6900629_Sprof.nc.\n",
      ">>> File 6900629_Sprof.nc already exists. Leaving current version.\n",
      ">>>> Destination file: /vortexfs1/share/boom/data/nc2pqt_test/GDAC/6900630_Sprof.nc.\n",
      ">>> File 6900630_Sprof.nc already exists. Leaving current version.\n",
      ">>>> Destination file: /vortexfs1/share/boom/data/nc2pqt_test/GDAC/6900631_Sprof.nc.\n",
      ">>> File 6900631_Sprof.nc already exists. Leaving current version.\n",
      ">>>> Destination file: /vortexfs1/share/boom/data/nc2pqt_test/GDAC/6900632_Sprof.nc.\n",
      ">>> File 6900632_Sprof.nc already exists. Leaving current version.\n",
      ">>>> Destination file: /vortexfs1/share/boom/data/nc2pqt_test/GDAC/6900889_Sprof.nc.\n",
      ">>> File 6900889_Sprof.nc already exists. Leaving current version.\n",
      ">>>> Destination file: /vortexfs1/share/boom/data/nc2pqt_test/GDAC/6900890_Sprof.nc.\n",
      ">>> File 6900890_Sprof.nc already exists. Leaving current version.\n",
      ">>>> Destination file: /vortexfs1/share/boom/data/nc2pqt_test/GDAC/6902984_Sprof.nc.\n",
      ">>> Successfully downloaded 6902984_Sprof.nc.\n",
      ">>>> Destination file: /vortexfs1/share/boom/data/nc2pqt_test/GDAC/6903067_Sprof.nc.\n",
      ">>> Successfully downloaded 6903067_Sprof.nc.\n",
      ">>>> Destination file: /vortexfs1/share/boom/data/nc2pqt_test/GDAC/6903091_Sprof.nc.\n",
      ">>> Successfully downloaded 6903091_Sprof.nc.\n",
      ">>>> Destination file: /vortexfs1/share/boom/data/nc2pqt_test/GDAC/6903876_Sprof.nc.\n",
      ">>> Successfully downloaded 6903876_Sprof.nc.\n",
      ">>>> Destination file: /vortexfs1/share/boom/data/nc2pqt_test/GDAC/6903877_Sprof.nc.\n",
      ">>> Successfully downloaded 6903877_Sprof.nc.\n",
      ">>>> Destination file: /vortexfs1/share/boom/data/nc2pqt_test/GDAC/6904134_Sprof.nc.\n",
      ">>> Successfully downloaded 6904134_Sprof.nc.\n",
      ">>>> Destination file: /vortexfs1/share/boom/data/nc2pqt_test/GDAC/6904139_Sprof.nc.\n",
      ">>> Successfully downloaded 6904139_Sprof.nc.\n",
      ">>>> Destination file: /vortexfs1/share/boom/data/nc2pqt_test/GDAC/7900559_Sprof.nc.\n",
      ">>> Successfully downloaded 7900559_Sprof.nc.\n",
      ">>>> Destination file: /vortexfs1/share/boom/data/nc2pqt_test/GDAC/7900560_Sprof.nc.\n",
      ">>> Successfully downloaded 7900560_Sprof.nc.\n",
      ">>>> Destination file: /vortexfs1/share/boom/data/nc2pqt_test/GDAC/7900561_Sprof.nc.\n",
      ">>> Successfully downloaded 7900561_Sprof.nc.\n",
      "All requested files have been downloaded.\n"
     ]
    }
   ],
   "source": [
    "wmos, df2, wmo_fp = at.argo_gdac(lat_range=[0,20],lon_range=[-30,-20],save_to=outdir_nc, download_individual_profs=False, dryrun=False, skip_downloads=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1901467_Sprof.nc',\n",
      " '1901468_Sprof.nc',\n",
      " '1901668_Sprof.nc',\n",
      " '4901283_Sprof.nc',\n",
      " '4901460_Sprof.nc',\n",
      " '6901174_Sprof.nc',\n",
      " '6901175_Sprof.nc',\n",
      " '1900650_Sprof.nc',\n",
      " '1900651_Sprof.nc',\n",
      " '1900652_Sprof.nc',\n",
      " '1900943_Sprof.nc',\n",
      " '1901360_Sprof.nc',\n",
      " '1901361_Sprof.nc',\n",
      " '1901363_Sprof.nc',\n",
      " '1901364_Sprof.nc',\n",
      " '1901365_Sprof.nc',\n",
      " '3900281_Sprof.nc',\n",
      " '3902122_Sprof.nc',\n",
      " '3902123_Sprof.nc',\n",
      " '6900524_Sprof.nc',\n",
      " '6900525_Sprof.nc',\n",
      " '6900628_Sprof.nc',\n",
      " '6900629_Sprof.nc',\n",
      " '6900630_Sprof.nc',\n",
      " '6900631_Sprof.nc',\n",
      " '6900632_Sprof.nc',\n",
      " '6900889_Sprof.nc',\n",
      " '6900890_Sprof.nc',\n",
      " '6902984_Sprof.nc',\n",
      " '6903067_Sprof.nc',\n",
      " '6903091_Sprof.nc',\n",
      " '6903876_Sprof.nc',\n",
      " '6903877_Sprof.nc',\n",
      " '6904134_Sprof.nc',\n",
      " '6904139_Sprof.nc',\n",
      " '7900559_Sprof.nc',\n",
      " '7900560_Sprof.nc',\n",
      " '7900561_Sprof.nc']\n"
     ]
    }
   ],
   "source": [
    "pprint(wmo_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing iterators from [`itertools`](https://docs.python.org/3/library/itertools.html)\n",
    "* `islice` returns selected elements from iterable\n",
    "* `batched` split the iterable object into tuples of prescribed length _n_ (if `length(iterable)%n~=0`, the last tuple is shorter than _n_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from itertools import islice\n",
    "\n",
    "if sys.version_info >= (3, 12):\n",
    "    from itertools import batched\n",
    "else:\n",
    "    try:\n",
    "        from more_itertools import batched\n",
    "    except ImportError:\n",
    "        def batched(iterable, chunk_size):\n",
    "            iterator = iter(iterable)\n",
    "            while chunk := tuple(islice(iterator, chunk_size)):\n",
    "                yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VARS = ['JULD','CYCLE_NUMBER','PLATFORM_NUMBER','LATITUDE','LONGITUDE',\n",
    " 'PRES',\n",
    " 'PRES_QC',\n",
    " 'PRES_ADJUSTED',\n",
    " 'PRES_ADJUSTED_QC',\n",
    " 'PRES_ADJUSTED_ERROR',\n",
    " 'TEMP',\n",
    " 'TEMP_QC',\n",
    " 'TEMP_dPRES',\n",
    " 'TEMP_ADJUSTED',\n",
    " 'TEMP_ADJUSTED_QC',\n",
    " 'TEMP_ADJUSTED_ERROR',\n",
    " 'PSAL',\n",
    " 'PSAL_QC',\n",
    " 'PSAL_dPRES',\n",
    " 'PSAL_ADJUSTED',\n",
    " 'PSAL_ADJUSTED_QC',\n",
    " 'PSAL_ADJUSTED_ERROR',\n",
    " 'DOXY',\n",
    " 'DOXY_QC',\n",
    " 'DOXY_dPRES',\n",
    " 'DOXY_ADJUSTED',\n",
    " 'DOXY_ADJUSTED_QC',\n",
    " 'DOXY_ADJUSTED_ERROR',\n",
    " 'CHLA',\n",
    " 'CHLA_QC',\n",
    " 'CHLA_dPRES',\n",
    " 'CHLA_ADJUSTED',\n",
    " 'CHLA_ADJUSTED_QC',\n",
    " 'CHLA_ADJUSTED_ERROR',\n",
    " 'BBP700',\n",
    " 'BBP700_QC',\n",
    " 'BBP700_dPRES',\n",
    " 'BBP700_ADJUSTED',\n",
    " 'BBP700_ADJUSTED_QC',\n",
    " 'BBP700_ADJUSTED_ERROR',\n",
    " 'CDOM',\n",
    " 'CDOM_QC',\n",
    " 'CDOM_dPRES',\n",
    " 'CDOM_ADJUSTED',\n",
    " 'CDOM_ADJUSTED_QC',\n",
    " 'CDOM_ADJUSTED_ERROR',\n",
    " 'PH_IN_SITU_TOTAL',\n",
    " 'PH_IN_SITU_TOTAL_QC',\n",
    " 'PH_IN_SITU_TOTAL_dPRES',\n",
    " 'PH_IN_SITU_TOTAL_ADJUSTED',\n",
    " 'PH_IN_SITU_TOTAL_ADJUSTED_QC',\n",
    " 'PH_IN_SITU_TOTAL_ADJUSTED_ERROR',\n",
    " 'NITRATE',\n",
    " 'NITRATE_QC',\n",
    " 'NITRATE_dPRES',\n",
    " 'NITRATE_ADJUSTED',\n",
    " 'NITRATE_ADJUSTED_QC',\n",
    " 'NITRATE_ADJUSTED_ERROR']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conversion from `nc` to `parquet` is parallelized. All you need to do is comment/uncomment the appropriate line in the next cell, selecting if you want to process the files that you just downloaded (first line) or other files (second line, modify to include your desired path).\n",
    "The new parquet files will be stored in the directory `outdir_pqt` that you specified earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "flist = [outdir_nc + s for s in wmo_fp]\n",
    "#flist = glob.glob(\"/vortexfs1/share/boom/data/gdac_snapshot/202403-ArgoData/dac/aoml/*/*_Sprof.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 38 files.\n",
      "\n",
      "Size per processor (MB)\n",
      "372.1313171386719\n",
      "\n",
      "372.1313171386719\n",
      "\n",
      "Size in processor 0 (MB):\n",
      "372.1313171386719\n",
      "\n",
      "Using 1 processors\n",
      "#0: processing file 10 of 38\n",
      "#0: processing file 20 of 38\n",
      "#0: processing file 30 of 38\n",
      "#0: Storing to parquet...\n",
      "#0: Filesize: 1.91 GB\n",
      "#0: /vortexfs1/share/boom/data/nc2pqt_test/PQT/test_parquet_0_0.parquet stored.\n",
      "CPU times: user 1min 35s, sys: 5.19 s, total: 1min 40s\n",
      "Wall time: 1min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "def xr2pqt(rank,files_list,loop_id):\n",
    "    df_list = []\n",
    "    df_memory = 0\n",
    "    counter = 0\n",
    "    rank_str = \"#\" + str(rank) + \": \"\n",
    "    nb_files = len(files_list)\n",
    "    for argo_file in files_list:\n",
    "        counter += 1\n",
    "        if counter%10==0:\n",
    "            print(rank_str + \"processing file \" + str(counter) + \" of \" + str(nb_files))\n",
    "            \n",
    "        try:\n",
    "            ds = xr.load_dataset(argo_file, engine=\"argo\") #loading into memory the profile\n",
    "        except:\n",
    "            print(rank_str + 'Failed on ' + str(argo_file))\n",
    "        \n",
    "        invars = list(set(VARS) & set(list(ds.data_vars)))\n",
    "        df = ds[invars].to_dataframe()\n",
    "        df_memory += df.memory_usage().sum()/(10**9) # tracking memory usage (in GB)\n",
    "        df_list.append(df)\n",
    "\n",
    "    # store to parquet once a large enough dataframe has been created\n",
    "    \n",
    "    print(rank_str + \"Storing to parquet...\")\n",
    "    print(rank_str + \"Filesize: \" + \"{:.2f}\".format(df_memory) + \" GB\")\n",
    "    \n",
    "    df_list = pd.concat(df_list)\n",
    "\n",
    "    parquet_filename = outdir_pqt + \"test_parquet_\" + str(rank) + \"_\" + str(loop_id) + \".parquet\"\n",
    "    df_list.to_parquet(parquet_filename)\n",
    "    print(rank_str + str(parquet_filename) + \" stored.\")\n",
    "\n",
    "    df_list = []\n",
    "    df_memory = 0\n",
    "    \n",
    "############################################################################################################\n",
    "\n",
    "def poolParams(flist):\n",
    "    size_flist = []\n",
    "    for f in flist:\n",
    "        size_flist.append( os.path.getsize(f)/1024**2 ) #size in MB\n",
    "    \n",
    "    size_tot = sum(size_flist)\n",
    "    NPROC = int(np.ceil(size_tot/400)) # Empirically, 300 MB of .nc files seems a good trade-off\n",
    "    size_per_proc = size_tot/NPROC\n",
    "\n",
    "    print('')\n",
    "    print('Size per processor (MB)')\n",
    "    print(size_per_proc)\n",
    "    print('')\n",
    "    \n",
    "    ids_sort = np.argsort(np.array(size_flist))\n",
    "    \n",
    "    chunks_ids = []\n",
    "    x = np.copy(ids_sort)\n",
    "    \n",
    "    for j in range(NPROC):\n",
    "        chunk_ids = []\n",
    "        chunk_size = 0\n",
    "        while ((chunk_size<size_per_proc) and (len(x) > 0)):\n",
    "            if len(chunk_ids)%2 == 0:\n",
    "                chunk_ids.append(x[-1])\n",
    "                x = x[:-1]\n",
    "            else:\n",
    "                chunk_ids.append(x[0])\n",
    "                x = x[1:]\n",
    "            chunk_size = sum(np.asarray(size_flist)[chunk_ids])\n",
    "        print(chunk_size)\n",
    "        chunks_ids.append(chunk_ids)\n",
    "    \n",
    "    if len(x) > 0:\n",
    "        warnings.warn(str(len(x)) + \" files have not been assigned to a processor.\")\n",
    "    \n",
    "    print('')\n",
    "    chunks=[]\n",
    "    skip_proc = 0\n",
    "    total_memory = 0\n",
    "    for j,chunk_ids in enumerate(chunks_ids):\n",
    "        print('Size in processor ' + str(j) + ' (MB):')\n",
    "        size_proc = sum(np.asarray(size_flist)[chunk_ids])\n",
    "        total_memory += size_proc\n",
    "        print(size_proc)\n",
    "        if size_proc == 0:\n",
    "            skip_proc += 1\n",
    "            continue\n",
    "        chunk = [flist[k] for k in chunk_ids]\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    NPROC -= skip_proc\n",
    "        \n",
    "    print('')\n",
    "    print(\"Using \" + str(NPROC) + \" processors\")\n",
    "    \n",
    "    return NPROC, chunks\n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "print(\"Processing \" + str(len(flist)) + \" files.\")\n",
    "\n",
    "NPROC, chunks = poolParams(flist)\n",
    "\n",
    "# fixing max nb of processes to prevent bottleneck likely due to I/O on disk queing operations and filling up the memory\n",
    "MAXPROC = 12\n",
    "\n",
    "if NPROC > MAXPROC:\n",
    "    print(\"Estimated number of processors might create bottleneck issues. Forcing to use \" + str(MAXPROC) + \" processors at a time.\")\n",
    "    # force to use at most MAXPROC processes, by looping over chunks\n",
    "    full_loops = NPROC//MAXPROC  #nb of loops to use at most MAXPROC\n",
    "    RESPROC = NPROC%MAXPROC   #nb of residual processors after the loops\n",
    "    pool_obj = multiprocessing.Pool(processes=MAXPROC)\n",
    "    i_start = 0\n",
    "    i_end   = 0\n",
    "    for full_loop in range(full_loops):\n",
    "        i_start = MAXPROC*full_loop\n",
    "        i_end   = MAXPROC*(full_loop+1)\n",
    "        pool_obj.starmap(xr2pqt, [(rank, chunk, full_loop) for rank, chunk in enumerate(chunks[i_start:i_end])] )\n",
    "    pool_obj.close()\n",
    "\n",
    "    # multiprocessing across residual processor pool with NPROC<MAXPROC\n",
    "    if RESPROC > 0:\n",
    "        pool_obj = multiprocessing.Pool(processes=RESPROC)\n",
    "        pool_obj.starmap(xr2pqt, [(rank, chunk, full_loop+1) for rank, chunk in enumerate(chunks[(i_end+1):])] )\n",
    "        pool_obj.close()\n",
    "\n",
    "elif NPROC > 1:\n",
    "    pool_obj = multiprocessing.Pool(processes=NPROC)\n",
    "    pool_obj.starmap(xr2pqt, [(rank, chunk, 0) for rank, chunk in enumerate(chunks)] )\n",
    "    pool_obj.close()\n",
    "\n",
    "else:\n",
    "    xr2pqt(0,flist,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test direct subsetting from parquet directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#t = pq.read_table(Path(outdir),filters=[('PRES', '<', 20),('LATITUDE', '<', 21.1),('LATITUDE', '>', 21)])\n",
    "t = pq.read_table(Path(outdir),filters=[('PLATFORM_NUMBER', '==', 1902304)])\n",
    "df = t.to_pandas()\n",
    "df\n",
    "#ds = xr.Dataset.from_dataframe(df)\n",
    "#ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Example loading Sprof from snapshot\n",
    "```\n",
    "ds = xr.load_dataset('/vortexfs1/share/boom/data/gdac_snapshot/202403-ArgoData/dac/aoml/1902304/1902304_Sprof.nc')\n",
    "df = ds.to_dataframe()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "from pyarrow import fs\n",
    "s3 = fs.S3FileSystem(region='us-east-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from pyarrow import Table\n",
    "\n",
    "ds = xr.load_dataset('/vortexfs1/share/boom/data/gdac_snapshot/202403-ArgoData/dac/aoml/1902304/1902304_Sprof.nc',engine=\"argo\")\n",
    "df = ds[['DOXY','PRES','NITRATE','PLATFORM_NUMBER']].to_dataframe()\n",
    "\n",
    "s3_filepath = 'data.parquet'\n",
    "\n",
    "pq.write_to_dataset(\n",
    "    Table.from_pandas(df),\n",
    "    s3_filepath,\n",
    "    filesystem=s3,\n",
    "    use_dictionary=True,\n",
    "    compression=\"snappy\",\n",
    "    version=\"2.4\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### single threaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "CHUNK_SZ = 100\n",
    "VARS = ['JULD','LATITUDE','LONGITUDE','PRES','PRES_ADJUSTED','DOXY_ADJUSTED','DOXY_ADJUSTED_QC','NITRATE','NITRATE_ADJUSTED','PSAL','TEMP','CYCLE_NUMBER','PLATFORM_NUMBER']\n",
    "for chunk in batched(wmo_fp,CHUNK_SZ):\n",
    "    dflist = []\n",
    "    for line in chunk:  \n",
    "        fn = (line.split('/')[1] + \"_Sprof.nc\")\n",
    "        fpath = Path(local_gdac) / \"dac\" / line / fn\n",
    "        try:\n",
    "            ds = xr.load_dataset(fpath)\n",
    "        except:\n",
    "            print(fpath)\n",
    "        invars = list(set(VARS) & set(list(ds.data_vars)))\n",
    "        df = ds[invars].to_dataframe()\n",
    "        dflist.append(df)\n",
    "    print(fpath)\n",
    "    df = pd.concat(dflist)\n",
    "    df.to_parquet('pq/test' + line.split('/')[1] + \".parquet\",coerce_timestamps='us',allow_truncated_timestamps=True)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
