{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import gsw\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import pyarrow.parquet as pq\n",
    "import requests as rq\n",
    "import os\n",
    "from urllib.parse import urljoin, urlencode\n",
    "import argo_tools as at\n",
    "import argopy\n",
    "from argopy import DataFetcher as ArgoDataFetcher\n",
    "local_gdac = '/vortexfs1/share/boom/data/gdac_snapshot/202403-ArgoData'\n",
    "#Path.mkdir('/vortexfs1/share/boom/projects/n2o/pq')\n",
    "outdir = '/vortexfs1/share/boom/projects/n2o/pq'\n",
    "argopy.set_options(mode='expert',src='gdac',ftp=local_gdac)\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "import glob\n",
    "import psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vortexfs1/home/enrico.milanese/projects/ARGO/nc2parquet/argo_tools.py:47: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "  gdac_index = pd.read_csv(gdac_path,delimiter=',',header=8,parse_dates=['date','date_update'],\n"
     ]
    }
   ],
   "source": [
    "wmos, df2, wmo_fp = at.argo_gdac()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array(['aoml/1900722/', 'aoml/1901378/', 'aoml/1901379/', ...,\n",
      "       'meds/4902553/', 'meds/4902554/', 'meds/4902555/'], dtype=object)\n"
     ]
    }
   ],
   "source": [
    "pprint(wmo_fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing iterators from [`itertools`](https://docs.python.org/3/library/itertools.html)\n",
    "* `islice` returns selected elements from iterable\n",
    "* `batched` split the iterable object into tuples of prescribed length _n_ (if `length(iterable)%n~=0`, the last tuple is shorter than _n_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from itertools import islice\n",
    "\n",
    "if sys.version_info >= (3, 12):\n",
    "    from itertools import batched\n",
    "else:\n",
    "    try:\n",
    "        from more_itertools import batched\n",
    "    except ImportError:\n",
    "        def batched(iterable, chunk_size):\n",
    "            iterator = iter(iterable)\n",
    "            while chunk := tuple(islice(iterator, chunk_size)):\n",
    "                yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VARS = ['JULD','CYCLE_NUMBER','PLATFORM_NUMBER','LATITUDE','LONGITUDE',\n",
    " 'PRES',\n",
    " 'PRES_QC',\n",
    " 'PRES_ADJUSTED',\n",
    " 'PRES_ADJUSTED_QC',\n",
    " 'PRES_ADJUSTED_ERROR',\n",
    " 'TEMP',\n",
    " 'TEMP_QC',\n",
    " 'TEMP_dPRES',\n",
    " 'TEMP_ADJUSTED',\n",
    " 'TEMP_ADJUSTED_QC',\n",
    " 'TEMP_ADJUSTED_ERROR',\n",
    " 'PSAL',\n",
    " 'PSAL_QC',\n",
    " 'PSAL_dPRES',\n",
    " 'PSAL_ADJUSTED',\n",
    " 'PSAL_ADJUSTED_QC',\n",
    " 'PSAL_ADJUSTED_ERROR',\n",
    " 'DOXY',\n",
    " 'DOXY_QC',\n",
    " 'DOXY_dPRES',\n",
    " 'DOXY_ADJUSTED',\n",
    " 'DOXY_ADJUSTED_QC',\n",
    " 'DOXY_ADJUSTED_ERROR',\n",
    " 'CHLA',\n",
    " 'CHLA_QC',\n",
    " 'CHLA_dPRES',\n",
    " 'CHLA_ADJUSTED',\n",
    " 'CHLA_ADJUSTED_QC',\n",
    " 'CHLA_ADJUSTED_ERROR',\n",
    " 'BBP700',\n",
    " 'BBP700_QC',\n",
    " 'BBP700_dPRES',\n",
    " 'BBP700_ADJUSTED',\n",
    " 'BBP700_ADJUSTED_QC',\n",
    " 'BBP700_ADJUSTED_ERROR',\n",
    " 'CDOM',\n",
    " 'CDOM_QC',\n",
    " 'CDOM_dPRES',\n",
    " 'CDOM_ADJUSTED',\n",
    " 'CDOM_ADJUSTED_QC',\n",
    " 'CDOM_ADJUSTED_ERROR',\n",
    " 'PH_IN_SITU_TOTAL',\n",
    " 'PH_IN_SITU_TOTAL_QC',\n",
    " 'PH_IN_SITU_TOTAL_dPRES',\n",
    " 'PH_IN_SITU_TOTAL_ADJUSTED',\n",
    " 'PH_IN_SITU_TOTAL_ADJUSTED_QC',\n",
    " 'PH_IN_SITU_TOTAL_ADJUSTED_ERROR',\n",
    " 'NITRATE',\n",
    " 'NITRATE_QC',\n",
    " 'NITRATE_dPRES',\n",
    " 'NITRATE_ADJUSTED',\n",
    " 'NITRATE_ADJUSTED_QC',\n",
    " 'NITRATE_ADJUSTED_ERROR']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 793 files.\n",
      "\n",
      "Size per processor (MB)\n",
      "277.2022272348404\n",
      "\n",
      "277.41802978515625\n",
      "280.36266231536865\n",
      "277.52312088012695\n",
      "280.22032356262207\n",
      "277.9414978027344\n",
      "278.5088586807251\n",
      "278.4192123413086\n",
      "267.22411251068115\n",
      "\n",
      "Size in processor 0 (MB):\n",
      "277.41802978515625\n",
      "Size in processor 1 (MB):\n",
      "280.36266231536865\n",
      "Size in processor 2 (MB):\n",
      "277.52312088012695\n",
      "Size in processor 3 (MB):\n",
      "280.22032356262207\n",
      "Size in processor 4 (MB):\n",
      "277.9414978027344\n",
      "Size in processor 5 (MB):\n",
      "278.5088586807251\n",
      "Size in processor 6 (MB):\n",
      "278.4192123413086\n",
      "Size in processor 7 (MB):\n",
      "267.22411251068115\n",
      "\n",
      "Using 8 processors\n",
      "#7: processing file 10 of 102\n",
      "#6: processing file 10 of 106\n",
      "#5: processing file 10 of 103\n",
      "#1: processing file 10 of 97\n",
      "#2: processing file 10 of 104\n",
      "#3: processing file 10 of 105\n",
      "#4: processing file 10 of 103\n",
      "#1: processing file 20 of 97\n",
      "#6: processing file 20 of 106\n",
      "#3: processing file 20 of 105\n",
      "#7: processing file 20 of 102\n",
      "#5: processing file 20 of 103\n",
      "#2: processing file 20 of 104\n",
      "#4: processing file 20 of 103\n",
      "#0: processing file 10 of 73\n",
      "#1: processing file 30 of 97\n",
      "#7: processing file 30 of 102\n",
      "#6: processing file 30 of 106\n",
      "#2: processing file 30 of 104\n",
      "#3: processing file 30 of 105\n",
      "#5: processing file 30 of 103\n",
      "#4: processing file 30 of 103\n",
      "#0: processing file 20 of 73\n",
      "#1: processing file 40 of 97\n",
      "#6: processing file 40 of 106\n",
      "#2: processing file 40 of 104\n",
      "#7: processing file 40 of 102\n",
      "#3: processing file 40 of 105\n",
      "#4: processing file 40 of 103\n",
      "#5: processing file 40 of 103\n",
      "#1: processing file 50 of 97\n",
      "#6: processing file 50 of 106\n",
      "#2: processing file 50 of 104\n",
      "#0: processing file 30 of 73\n",
      "#7: processing file 50 of 102\n",
      "#3: processing file 50 of 105\n",
      "#5: processing file 50 of 103\n",
      "#4: processing file 50 of 103\n",
      "#1: processing file 60 of 97\n",
      "#6: processing file 60 of 106\n",
      "#2: processing file 60 of 104\n",
      "#7: processing file 60 of 102\n",
      "#3: processing file 60 of 105\n",
      "#5: processing file 60 of 103\n",
      "#0: processing file 40 of 73\n",
      "#4: processing file 60 of 103\n",
      "#1: processing file 70 of 97\n",
      "#6: processing file 70 of 106\n",
      "#2: processing file 70 of 104\n",
      "#7: processing file 70 of 102\n",
      "#3: processing file 70 of 105\n",
      "#0: processing file 50 of 73\n",
      "#5: processing file 70 of 103\n",
      "#4: processing file 70 of 103\n",
      "#1: processing file 80 of 97\n",
      "#6: processing file 80 of 106\n",
      "#2: processing file 80 of 104\n",
      "#7: processing file 80 of 102\n",
      "#3: processing file 80 of 105\n",
      "#5: processing file 80 of 103\n",
      "#4: processing file 80 of 103\n",
      "#0: processing file 60 of 73\n",
      "#6: processing file 90 of 106\n",
      "#1: processing file 90 of 97\n",
      "#2: processing file 90 of 104\n",
      "#7: processing file 90 of 102\n",
      "#3: processing file 90 of 105\n",
      "#5: processing file 90 of 103\n",
      "#1: Storing to parquet...\n",
      "#1: Filesize: 1.72 GB\n",
      "#4: processing file 90 of 103\n",
      "#0: processing file 70 of 73\n",
      "#6: processing file 100 of 106\n",
      "#2: processing file 100 of 104\n",
      "#0: Storing to parquet...\n",
      "#0: Filesize: 1.65 GB\n",
      "#7: processing file 100 of 102\n",
      "#3: processing file 100 of 105\n",
      "#5: processing file 100 of 103\n",
      "#2: Storing to parquet...\n",
      "#2: Filesize: 1.78 GB\n",
      "#6: Storing to parquet...\n",
      "#6: Filesize: 1.67 GB\n",
      "#7: Storing to parquet...\n",
      "#7: Filesize: 1.73 GB\n",
      "#4: processing file 100 of 103\n",
      "#5: Storing to parquet...\n",
      "#5: Filesize: 1.81 GB\n",
      "#3: Storing to parquet...\n",
      "#3: Filesize: 1.79 GB\n",
      "#4: Storing to parquet...\n",
      "#4: Filesize: 1.73 GB\n",
      "#1: /vortexfs1/share/boom/data/nc2pqt_test/test_parquet_1.parquet stored.\n",
      "#0: /vortexfs1/share/boom/data/nc2pqt_test/test_parquet_0.parquet stored.\n",
      "#7: /vortexfs1/share/boom/data/nc2pqt_test/test_parquet_7.parquet stored.\n",
      "#6: /vortexfs1/share/boom/data/nc2pqt_test/test_parquet_6.parquet stored.\n",
      "#2: /vortexfs1/share/boom/data/nc2pqt_test/test_parquet_2.parquet stored.\n",
      "#4: /vortexfs1/share/boom/data/nc2pqt_test/test_parquet_4.parquet stored.\n",
      "#5: /vortexfs1/share/boom/data/nc2pqt_test/test_parquet_5.parquet stored.\n",
      "#3: /vortexfs1/share/boom/data/nc2pqt_test/test_parquet_3.parquet stored.\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "def xr2pqt(rank,files_list):\n",
    "    df_list = []\n",
    "    df_memory = 0\n",
    "    counter = 0\n",
    "    rank_str = \"#\" + str(rank) + \": \"\n",
    "    nb_files = len(files_list)\n",
    "    for argo_file in files_list:\n",
    "        counter += 1\n",
    "        if counter%10==0:\n",
    "            print(rank_str + \"processing file \" + str(counter) + \" of \" + str(nb_files))\n",
    "            \n",
    "        try:\n",
    "            ds = xr.load_dataset(argo_file, engine=\"argo\") #loading into memory the profile\n",
    "        except:\n",
    "            print(rank_str + 'Failed on ' + str(argo_file))\n",
    "        \n",
    "        invars = list(set(VARS) & set(list(ds.data_vars)))\n",
    "        df = ds[invars].to_dataframe()\n",
    "        df_memory += df.memory_usage().sum()/(10**9) # tracking memory usage (in GB)\n",
    "        df_list.append(df)\n",
    "\n",
    "        # print(rank_str + \"{:.3f}\".format(df_memory))\n",
    "\n",
    "    # store to parquet once a large enough dataframe has been created\n",
    "    \n",
    "    print(rank_str + \"Storing to parquet...\")\n",
    "    print(rank_str + \"Filesize: \" + \"{:.2f}\".format(df_memory) + \" GB\")\n",
    "    \n",
    "    df_list = pd.concat(df_list)\n",
    "\n",
    "    # root_storage = \"/vortexfs1/home/enrico.milanese/projects/ARGO/nc2parquet/PQT_test/test_parquet_\"\n",
    "    root_storage = \"/vortexfs1/share/boom/data/nc2pqt_test/\"\n",
    "    parquet_filename = root_storage + \"test_parquet_\" + str(rank) + \".parquet\"\n",
    "    df_list.to_parquet(parquet_filename)\n",
    "    print(rank_str + str(parquet_filename) + \" stored.\")\n",
    "\n",
    "    # print(\"Storing to csv...\")\n",
    "    # csv_filename = \"/vortexfs1/home/enrico.milanese/projects/ARGO/nc2parquet/PQT_test/test_parquet_\" + str(pqt_files) + \".csv\"\n",
    "    # df_list.to_csv(csv_filename)\n",
    "    # print(str(csv_filename) + \" stored.\")\n",
    "    df_list = []\n",
    "    df_memory = 0\n",
    "    \n",
    "############################################################################################################\n",
    "\n",
    "def poolParams(flist):\n",
    "    size_flist = []\n",
    "    for f in flist:\n",
    "        size_flist.append( os.path.getsize(f)/1024**2 ) #size in MB\n",
    "    \n",
    "    size_tot = sum(size_flist)\n",
    "    NPROC = int(np.ceil(size_tot/300)) # Empirically, 300 MB of .nc files seems a good trade-off\n",
    "    size_per_proc = size_tot/NPROC\n",
    "\n",
    "    print('')\n",
    "    print('Size per processor (MB)')\n",
    "    print(size_per_proc)\n",
    "    print('')\n",
    "    \n",
    "    ids_sort = np.argsort(np.array(size_flist))\n",
    "    \n",
    "    chunks_ids = []\n",
    "    x = np.copy(ids_sort)\n",
    "    \n",
    "    for j in range(NPROC):\n",
    "        chunk_ids = []\n",
    "        chunk_size = 0\n",
    "        while ((chunk_size<size_per_proc) and (len(x) > 0)):\n",
    "            if len(chunk_ids)%2 == 0:\n",
    "                chunk_ids.append(x[-1])\n",
    "                x = x[:-1]\n",
    "            else:\n",
    "                chunk_ids.append(x[0])\n",
    "                x = x[1:]\n",
    "            chunk_size = sum(np.asarray(size_flist)[chunk_ids])\n",
    "        print(chunk_size)\n",
    "        chunks_ids.append(chunk_ids)\n",
    "    \n",
    "    if len(x) > 0:\n",
    "        warnings.warn(str(len(x)) + \" files have not been assigned to a processor.\")\n",
    "    \n",
    "    print('')\n",
    "    chunks=[]\n",
    "    skip_proc = 0\n",
    "    total_memory = 0\n",
    "    for j,chunk_ids in enumerate(chunks_ids):\n",
    "        print('Size in processor ' + str(j) + ' (MB):')\n",
    "        size_proc = sum(np.asarray(size_flist)[chunk_ids])\n",
    "        total_memory += size_proc\n",
    "        print(size_proc)\n",
    "        if size_proc == 0:\n",
    "            skip_proc += 1\n",
    "            continue\n",
    "        chunk = [flist[k] for k in chunk_ids]\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    NPROC -= skip_proc\n",
    "        \n",
    "    print('')\n",
    "    print(\"Using \" + str(NPROC) + \" processors\")\n",
    "    \n",
    "    return NPROC, chunks\n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "# wmos, df2, wmo_fp = at.argo_gdac()\n",
    "\n",
    "# flist = []\n",
    "# for line in wmo_fp:  \n",
    "#         if '6902958' in line:\n",
    "#             print('skipping 6902958')\n",
    "#         else:\n",
    "#             fn = (line.split('/')[1] + \"_Sprof.nc\")\n",
    "#             fpath = Path(local_gdac) / \"dac\" / line / fn\n",
    "#             if os.path.isfile(fpath):\n",
    "#                 flist.append(str(fpath))\n",
    "#             else:\n",
    "#                 warnings.warn(\"File \" + str(fpath) + \" not found, skipping it.\")\n",
    "\n",
    "# print(flist)\n",
    "\n",
    "# flist = glob.glob(\"/vortexfs1/home/enrico.milanese/projects/ARGO/nc2parquet/GDAC/pub/dac/aoml/*/*_Sprof.nc\")\n",
    "flist = glob.glob(\"/vortexfs1/share/boom/data/gdac_snapshot/202403-ArgoData/dac/aoml/*/*_Sprof.nc\")\n",
    "\n",
    "print(\"Processing \" + str(len(flist)) + \" files.\")\n",
    "\n",
    "NPROC, chunks = poolParams(flist)\n",
    "\n",
    "pool_obj = multiprocessing.Pool(processes=NPROC)\n",
    "pool_obj.starmap(xr2pqt, [(rank, chunk) for rank, chunk in enumerate(chunks)] )\n",
    "pool_obj.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test direct subsetting from parquet directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#t = pq.read_table(Path(outdir),filters=[('PRES', '<', 20),('LATITUDE', '<', 21.1),('LATITUDE', '>', 21)])\n",
    "t = pq.read_table(Path(outdir),filters=[('PLATFORM_NUMBER', '==', 1902304)])\n",
    "df = t.to_pandas()\n",
    "df\n",
    "#ds = xr.Dataset.from_dataframe(df)\n",
    "#ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Example loading Sprof from snapshot\n",
    "```\n",
    "ds = xr.load_dataset('/vortexfs1/share/boom/data/gdac_snapshot/202403-ArgoData/dac/aoml/1902304/1902304_Sprof.nc')\n",
    "df = ds.to_dataframe()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "from pyarrow import fs\n",
    "s3 = fs.S3FileSystem(region='us-east-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from pyarrow import Table\n",
    "\n",
    "ds = xr.load_dataset('/vortexfs1/share/boom/data/gdac_snapshot/202403-ArgoData/dac/aoml/1902304/1902304_Sprof.nc',engine=\"argo\")\n",
    "df = ds[['DOXY','PRES','NITRATE','PLATFORM_NUMBER']].to_dataframe()\n",
    "\n",
    "s3_filepath = 'data.parquet'\n",
    "\n",
    "pq.write_to_dataset(\n",
    "    Table.from_pandas(df),\n",
    "    s3_filepath,\n",
    "    filesystem=s3,\n",
    "    use_dictionary=True,\n",
    "    compression=\"snappy\",\n",
    "    version=\"2.4\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### single threaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "CHUNK_SZ = 100\n",
    "VARS = ['JULD','LATITUDE','LONGITUDE','PRES','PRES_ADJUSTED','DOXY_ADJUSTED','DOXY_ADJUSTED_QC','NITRATE','NITRATE_ADJUSTED','PSAL','TEMP','CYCLE_NUMBER','PLATFORM_NUMBER']\n",
    "for chunk in batched(wmo_fp,CHUNK_SZ):\n",
    "    dflist = []\n",
    "    for line in chunk:  \n",
    "        fn = (line.split('/')[1] + \"_Sprof.nc\")\n",
    "        fpath = Path(local_gdac) / \"dac\" / line / fn\n",
    "        try:\n",
    "            ds = xr.load_dataset(fpath)\n",
    "        except:\n",
    "            print(fpath)\n",
    "        invars = list(set(VARS) & set(list(ds.data_vars)))\n",
    "        df = ds[invars].to_dataframe()\n",
    "        dflist.append(df)\n",
    "    print(fpath)\n",
    "    df = pd.concat(dflist)\n",
    "    df.to_parquet('pq/test' + line.split('/')[1] + \".parquet\",coerce_timestamps='us',allow_truncated_timestamps=True)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
