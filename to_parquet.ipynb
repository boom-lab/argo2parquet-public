{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import gsw\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import pyarrow.parquet as pq\n",
    "import requests as rq\n",
    "import os\n",
    "from urllib.parse import urljoin, urlencode\n",
    "import argo_tools as at\n",
    "import argopy\n",
    "from argopy import DataFetcher as ArgoDataFetcher\n",
    "gdac_path = '/vortexfs1/share/boom/data/nc2pqt_test/'\n",
    "outdir_nc = '/vortexfs1/share/boom/data/nc2pqt_test/GDAC/dac/'\n",
    "# gdac_path = '/vortexfs1/home/enrico.milanese/projects/ARGO/nc2parquet/'\n",
    "# outdir_nc = '/vortexfs1/home/enrico.milanese/projects/ARGO/nc2parquet/GDAC_test/'\n",
    "\n",
    "outdir_pqt = '/vortexfs1/share/boom/data/nc2pqt_test/PQT/'\n",
    "# argopy.set_options(mode='expert',src='gdac',ftp=local_gdac)\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "import glob\n",
    "import psutil\n",
    "import time\n",
    "import multiprocessing\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downloading Argo profiles\n",
    "\n",
    "If you already have the profiles stored somewhere, you can skip this. Otherwise, if you want to download them, uncomment the next cell, and pass to `at.argo_dac` the appropriate arguments to download the files you desire (see `argo_tools.py` for details about the arguments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vortexfs1/home/enrico.milanese/projects/ARGO/nc2parquet/argo_tools.py:77: FutureWarning: The argument 'date_parser' is deprecated and will be removed in a future version. Please use 'date_format' instead, or read your data in as 'object' dtype and then call 'to_datetime'.\n",
      "  gdac_index = pd.read_csv(gdac_file,delimiter=',',header=8,parse_dates=['date','date_update'],\n"
     ]
    }
   ],
   "source": [
    "wmos, df2, wmo_fp = at.argo_gdac(gdac_path=gdac_path,save_to=outdir_nc, download_individual_profs=False, skip_downloads=False, dryrun=True, checktime=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing iterators from [`itertools`](https://docs.python.org/3/library/itertools.html)\n",
    "* `islice` returns selected elements from iterable\n",
    "* `batched` split the iterable object into tuples of prescribed length _n_ (if `length(iterable)%n~=0`, the last tuple is shorter than _n_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import itertools\n",
    "from itertools import islice\n",
    "\n",
    "if sys.version_info >= (3, 12):\n",
    "    from itertools import batched\n",
    "else:\n",
    "    try:\n",
    "        from more_itertools import batched\n",
    "    except ImportError:\n",
    "        def batched(iterable, chunk_size):\n",
    "            iterator = iter(iterable)\n",
    "            while chunk := tuple(islice(iterator, chunk_size)):\n",
    "                yield chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "VARS = ['LATITUDE','LONGITUDE','JULD','CYCLE_NUMBER','PLATFORM_NUMBER','LATITUDE','LONGITUDE',\n",
    " 'PRES',\n",
    " 'PRES_QC',\n",
    " 'PRES_ADJUSTED',\n",
    " 'PRES_ADJUSTED_QC',\n",
    " 'PRES_ADJUSTED_ERROR',\n",
    " 'TEMP',\n",
    " 'TEMP_QC',\n",
    " 'TEMP_dPRES',\n",
    " 'TEMP_ADJUSTED',\n",
    " 'TEMP_ADJUSTED_QC',\n",
    " 'TEMP_ADJUSTED_ERROR',\n",
    " 'PSAL',\n",
    " 'PSAL_QC',\n",
    " 'PSAL_dPRES',\n",
    " 'PSAL_ADJUSTED',\n",
    " 'PSAL_ADJUSTED_QC',\n",
    " 'PSAL_ADJUSTED_ERROR',\n",
    " 'DOXY',\n",
    " 'DOXY_QC',\n",
    " 'DOXY_dPRES',\n",
    " 'DOXY_ADJUSTED',\n",
    " 'DOXY_ADJUSTED_QC',\n",
    " 'DOXY_ADJUSTED_ERROR',\n",
    " 'CHLA',\n",
    " 'CHLA_QC',\n",
    " 'CHLA_dPRES',\n",
    " 'CHLA_ADJUSTED',\n",
    " 'CHLA_ADJUSTED_QC',\n",
    " 'CHLA_ADJUSTED_ERROR',\n",
    " 'BBP700',\n",
    " 'BBP700_QC',\n",
    " 'BBP700_dPRES',\n",
    " 'BBP700_ADJUSTED',\n",
    " 'BBP700_ADJUSTED_QC',\n",
    " 'BBP700_ADJUSTED_ERROR',\n",
    " 'CDOM',\n",
    " 'CDOM_QC',\n",
    " 'CDOM_dPRES',\n",
    " 'CDOM_ADJUSTED',\n",
    " 'CDOM_ADJUSTED_QC',\n",
    " 'CDOM_ADJUSTED_ERROR',\n",
    " 'PH_IN_SITU_TOTAL',\n",
    " 'PH_IN_SITU_TOTAL_QC',\n",
    " 'PH_IN_SITU_TOTAL_dPRES',\n",
    " 'PH_IN_SITU_TOTAL_ADJUSTED',\n",
    " 'PH_IN_SITU_TOTAL_ADJUSTED_QC',\n",
    " 'PH_IN_SITU_TOTAL_ADJUSTED_ERROR',\n",
    " 'NITRATE',\n",
    " 'NITRATE_QC',\n",
    " 'NITRATE_dPRES',\n",
    " 'NITRATE_ADJUSTED',\n",
    " 'NITRATE_ADJUSTED_QC',\n",
    " 'NITRATE_ADJUSTED_ERROR']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File conversion\n",
    "\n",
    "The conversion from `nc` to `parquet` is parallelized. All you need to do is comment/uncomment the appropriate line in the next cell, selecting if you want to process the files that you just downloaded (first line) or other files (second line, modify to include your desired path).\n",
    "The new parquet files will be stored in the directory `outdir_pqt` that you specified earlier.\n",
    "\n",
    "The default is for the code to execute over multiple processes. If you want to execute it on a single thread, set `single_process = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "flist = wmo_fp\n",
    "single_process = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/vortexfs1/share/boom/data/nc2pqt_test/PQT/metadata/test_metadata.parquet stored.\n",
      "Processing 2239 files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vortexfs1/home/enrico.milanese/projects/ARGO/nc2parquet/venv/venv3.9/lib/python3.9/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host 'usgodae.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /vortexfs1/share/boom/data/nc2pqt_test/GDAC/dac/coriolis/3902104/3902104_Sprof.nc returned 404 error from URL https://usgodae.org/pub/outgoing/argo/dac/coriolis/3902104/3902104_Sprof.nc. Skipping it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vortexfs1/home/enrico.milanese/projects/ARGO/nc2parquet/venv/venv3.9/lib/python3.9/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host 'usgodae.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /vortexfs1/share/boom/data/nc2pqt_test/GDAC/dac/coriolis/3902115/3902115_Sprof.nc returned 404 error from URL https://usgodae.org/pub/outgoing/argo/dac/coriolis/3902115/3902115_Sprof.nc. Skipping it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vortexfs1/home/enrico.milanese/projects/ARGO/nc2parquet/venv/venv3.9/lib/python3.9/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host 'usgodae.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /vortexfs1/share/boom/data/nc2pqt_test/GDAC/dac/coriolis/7900586/7900586_Sprof.nc returned 404 error from URL https://usgodae.org/pub/outgoing/argo/dac/coriolis/7900586/7900586_Sprof.nc. Skipping it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vortexfs1/home/enrico.milanese/projects/ARGO/nc2parquet/venv/venv3.9/lib/python3.9/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host 'usgodae.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /vortexfs1/share/boom/data/nc2pqt_test/GDAC/dac/coriolis/7900587/7900587_Sprof.nc returned 404 error from URL https://usgodae.org/pub/outgoing/argo/dac/coriolis/7900587/7900587_Sprof.nc. Skipping it.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vortexfs1/home/enrico.milanese/projects/ARGO/nc2parquet/venv/venv3.9/lib/python3.9/site-packages/urllib3/connectionpool.py:1013: InsecureRequestWarning: Unverified HTTPS request is being made to host 'usgodae.org'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /vortexfs1/share/boom/data/nc2pqt_test/GDAC/dac/csio/2902836/2902836_Sprof.nc returned 404 error from URL https://usgodae.org/pub/outgoing/argo/dac/csio/2902836/2902836_Sprof.nc. Skipping it.\n",
      "\n",
      "Size per processor (MB)\n",
      "39.96117754656859\n",
      "\n",
      "99.08544540405273\n",
      "97.73884105682373\n",
      "96.52665042877197\n",
      "89.80551052093506\n",
      "87.98799419403076\n",
      "87.86990928649902\n",
      "87.43379592895508\n",
      "85.42619132995605\n",
      "83.34097290039062\n",
      "82.52131366729736\n",
      "81.72994232177734\n",
      "81.44808578491211\n",
      "81.2982702255249\n",
      "80.78289413452148\n",
      "79.62730979919434\n",
      "76.59406471252441\n",
      "72.35435104370117\n",
      "69.35292625427246\n",
      "65.87495422363281\n",
      "65.13488674163818\n",
      "63.51381015777588\n",
      "61.91689109802246\n",
      "57.574278831481934\n",
      "57.280272483825684\n",
      "56.58053207397461\n",
      "54.36483383178711\n",
      "52.90372276306152\n",
      "52.17085838317871\n",
      "51.750932693481445\n",
      "50.643943786621094\n",
      "50.5104455947876\n",
      "48.93765640258789\n",
      "48.876712799072266\n",
      "48.60991191864014\n",
      "48.206050872802734\n",
      "47.92483425140381\n",
      "47.206130027770996\n",
      "46.32355880737305\n",
      "45.93830585479736\n",
      "44.92294502258301\n",
      "44.75474262237549\n",
      "44.4288272857666\n",
      "44.13960933685303\n",
      "43.012332916259766\n",
      "42.68037509918213\n",
      "42.51829433441162\n",
      "42.28854179382324\n",
      "41.75073051452637\n",
      "41.32218933105469\n",
      "41.2997350692749\n",
      "40.956759452819824\n",
      "79.14640235900879\n",
      "78.31860065460205\n",
      "76.42755794525146\n",
      "76.03366184234619\n",
      "75.22906684875488\n",
      "73.85813903808594\n",
      "71.42353248596191\n",
      "68.80791664123535\n",
      "67.4130802154541\n",
      "65.21195125579834\n",
      "63.610578536987305\n",
      "59.69996452331543\n",
      "56.37656211853027\n",
      "55.770362854003906\n",
      "55.36014938354492\n",
      "52.9484224319458\n",
      "52.1029691696167\n",
      "51.82318305969238\n",
      "51.36530780792236\n",
      "50.79065990447998\n",
      "50.438798904418945\n",
      "49.64658260345459\n",
      "48.09321594238281\n",
      "46.19425582885742\n",
      "45.44039249420166\n",
      "44.55861473083496\n",
      "44.27334690093994\n",
      "41.89920997619629\n",
      "41.32064437866211\n",
      "40.43600845336914\n",
      "40.100534439086914\n",
      "59.32606220245361\n",
      "58.48970985412598\n",
      "57.155479431152344\n",
      "56.598599433898926\n",
      "55.59438991546631\n",
      "55.26577568054199\n",
      "54.95966815948486\n",
      "54.5049524307251\n",
      "53.621389389038086\n",
      "53.007304191589355\n",
      "52.36145305633545\n",
      "51.430562019348145\n",
      "49.87660598754883\n",
      "48.20090293884277\n",
      "47.1083984375\n",
      "46.16872024536133\n",
      "43.61573314666748\n",
      "42.17696475982666\n",
      "41.67385196685791\n",
      "41.32375907897949\n",
      "40.53586483001709\n",
      "39.9703426361084\n",
      "52.39725589752197\n",
      "51.9581184387207\n",
      "50.37189960479736\n",
      "49.29390525817871\n",
      "48.36229419708252\n",
      "48.15830898284912\n",
      "47.70870780944824\n",
      "46.583224296569824\n",
      "44.52020740509033\n",
      "43.16057014465332\n",
      "42.15037727355957\n",
      "41.38874053955078\n",
      "41.04922962188721\n",
      "40.6306037902832\n",
      "40.12253952026367\n",
      "48.94076919555664\n",
      "47.399166107177734\n",
      "46.45067119598389\n",
      "46.13659381866455\n",
      "45.948073387145996\n",
      "45.75635242462158\n",
      "45.228875160217285\n",
      "44.66277599334717\n",
      "44.36042404174805\n",
      "44.0448522567749\n",
      "43.64846324920654\n",
      "43.1279354095459\n",
      "42.50391960144043\n",
      "41.90567493438721\n",
      "41.50613212585449\n",
      "40.80646514892578\n",
      "40.21747970581055\n",
      "40.03910446166992\n",
      "47.07423973083496\n",
      "46.32049369812012\n",
      "45.633301734924316\n",
      "44.953673362731934\n",
      "44.185136795043945\n",
      "43.47554874420166\n",
      "42.06228446960449\n",
      "41.281015396118164\n",
      "40.88961219787598\n",
      "40.33890724182129\n",
      "40.31794738769531\n",
      "40.0287446975708\n",
      "45.919355392456055\n",
      "45.43899059295654\n",
      "45.00569534301758\n",
      "44.46884727478027\n",
      "43.871421813964844\n",
      "43.58131980895996\n",
      "43.21565914154053\n",
      "42.90809345245361\n",
      "42.512911796569824\n",
      "42.24337673187256\n",
      "42.0512056350708\n",
      "41.78731441497803\n",
      "41.317986488342285\n",
      "41.01858043670654\n",
      "40.6785364151001\n",
      "40.45440673828125\n",
      "40.0662956237793\n",
      "40.093491554260254\n",
      "45.064215660095215\n",
      "44.67565059661865\n",
      "44.11093235015869\n",
      "43.89711380004883\n",
      "43.794036865234375\n",
      "43.52293014526367\n",
      "43.213369369506836\n",
      "42.693359375\n",
      "42.365264892578125\n",
      "41.957075119018555\n",
      "41.79678249359131\n",
      "41.58424377441406\n",
      "41.50131607055664\n",
      "41.18354797363281\n",
      "40.92381191253662\n",
      "40.74890613555908\n",
      "40.63400936126709\n",
      "40.5173397064209\n",
      "40.393404960632324\n",
      "40.23720741271973\n",
      "40.56154918670654\n",
      "40.22273254394531\n",
      "40.04157257080078\n",
      "44.050601959228516\n",
      "43.83548831939697\n",
      "43.585957527160645\n",
      "43.31523513793945\n",
      "42.9935302734375\n",
      "42.84348964691162\n",
      "42.602102279663086\n",
      "42.42743396759033\n",
      "42.07847595214844\n",
      "41.68805980682373\n",
      "40.978912353515625\n",
      "40.71104431152344\n",
      "40.360995292663574\n",
      "40.17386531829834\n",
      "40.53385066986084\n",
      "40.14840126037598\n",
      "43.45685386657715\n",
      "42.89071273803711\n",
      "42.61990547180176\n",
      "42.12067222595215\n",
      "41.64707279205322\n",
      "41.05277061462402\n",
      "40.570204734802246\n",
      "40.3248233795166\n",
      "40.823384284973145\n",
      "40.55857181549072\n",
      "40.37393569946289\n",
      "42.69072437286377\n",
      "42.4037561416626\n",
      "42.25358581542969\n",
      "41.652883529663086\n",
      "41.53340148925781\n",
      "41.05446815490723\n",
      "41.05258846282959\n",
      "40.96086025238037\n",
      "40.68978977203369\n",
      "40.518988609313965\n",
      "40.430715560913086\n",
      "40.519775390625\n",
      "40.39777088165283\n",
      "40.34046268463135\n",
      "40.2659797668457\n",
      "40.230926513671875\n",
      "40.24018096923828\n",
      "40.21019172668457\n",
      "41.515127182006836\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:144\u001b[0m\n",
      "File \u001b[0;32m<timed exec>:103\u001b[0m, in \u001b[0;36mpoolParams\u001b[0;34m(flist, nc_size_per_pqt)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "if not single_process:\n",
    "    import multiprocessing\n",
    "\n",
    "def xr2pqt(rank,files_list,loop_id):\n",
    "    df_list = []\n",
    "    df_memory = 0\n",
    "    counter = 0\n",
    "    rank_str = \"#\" + str(rank) + \": \"\n",
    "    nb_files = len(files_list)\n",
    "    argo_file_fail = []\n",
    "    for argo_file in files_list:\n",
    "        counter += 1\n",
    "        if counter%10==0:\n",
    "            print(rank_str + \"processing file \" + str(counter) + \" of \" + str(nb_files))\n",
    "            \n",
    "        try:\n",
    "            ds = xr.load_dataset(argo_file, engine=\"argo\") #loading into memory the profile\n",
    "        except:\n",
    "            print(rank_str + 'Failed on ' + str(argo_file))\n",
    "            argo_file_fail.append(argo_file)\n",
    "            continue\n",
    "        \n",
    "        invars = list(set(VARS) & set(list(ds.data_vars))) # some floats don't have all the vars specified in VARS\n",
    "        df = ds[invars].to_dataframe()\n",
    "\n",
    "        if not df.empty:\n",
    "            for col in VARS:\n",
    "                if col not in invars:\n",
    "                    df[col] = np.nan #ensures that all parquet files have all the VARS as columns\n",
    "            df_memory += df.memory_usage().sum()/(10**6) # tracking memory usage (in MB)\n",
    "            df_list.append(df)\n",
    "\n",
    "    # store to parquet once a large enough dataframe has been created\n",
    "    print(rank_str + \"Storing to parquet...\")\n",
    "    if df_memory < 1e3:\n",
    "        print(rank_str + \"In-memory filesize: \" + \"{:.2f}\".format(df_memory) + \" MB\")\n",
    "    else:\n",
    "        print(rank_str + \"In-memory filesize: \" + \"{:.2f}\".format(df_memory/1e3) + \" GB\")\n",
    "\n",
    "    try:\n",
    "        df_list = pd.concat(df_list)\n",
    "    except:\n",
    "        print(rank_str + 'Could not concatenate pandas dataframes')\n",
    "        print(rank_str + 'Failed on ' + str(argo_file) + '. Caution: more files might be affected.')\n",
    "        print(rank_str + 'Data frames list:')\n",
    "        print(df_list)\n",
    "        return argo_file_fail    \n",
    "        \n",
    "    parquet_filename = outdir_pqt + \"test_profiles_levels_\" + str(rank) + \"_\" + str(loop_id) + \".parquet\"\n",
    "    df_list.to_parquet(parquet_filename)\n",
    "    print(rank_str + str(parquet_filename) + \" stored.\")\n",
    "\n",
    "    df_list = []\n",
    "    df_memory = 0\n",
    "\n",
    "    return argo_file_fail\n",
    "    \n",
    "############################################################################################################\n",
    "\n",
    "def poolParams(flist,nc_size_per_pqt):\n",
    "    size_flist = []\n",
    "    for f in flist:\n",
    "        try:\n",
    "            f_size = os.path.getsize(f)/1024**2\n",
    "            size_flist.append( f_size ) #size in MB\n",
    "        except:\n",
    "            if not os.path.isfile(f):\n",
    "                gdac_root = 'https://usgodae.org/pub/outgoing/argo/dac/'\n",
    "                fpath = os.path.join( *f.split(os.path.sep)[-3:] )\n",
    "                response = at.get_func( gdac_root + fpath )\n",
    "                if response.status_code == 404:\n",
    "                    print('File ' + f + ' returned 404 error from URL ' + str(gdac_root+fpath) + '. Skipping it.')\n",
    "                else:\n",
    "                    print('File ' + f + ' likely present at URL ' + str(gdac_root+fpath) + '. You might want to check why it is not in the local drive.')\n",
    "            else:\n",
    "                print('File ' + f + ' seems to exist in the local drive: not sure what is going on here.')\n",
    "            continue\n",
    "            \n",
    "    size_tot = sum(size_flist)\n",
    "    NPROC = int(np.ceil(size_tot/nc_size_per_pqt))\n",
    "    size_per_proc = size_tot/NPROC\n",
    "\n",
    "    print('')\n",
    "    print('Size per processor (MB)')\n",
    "    print(size_per_proc)\n",
    "    print('')\n",
    "    \n",
    "    ids_sort = np.argsort(np.array(size_flist))\n",
    "    \n",
    "    chunks_ids = []\n",
    "    x = np.copy(ids_sort)\n",
    "    \n",
    "    for j in range(NPROC):\n",
    "        chunk_ids = []\n",
    "        chunk_size = 0\n",
    "        while ((chunk_size<size_per_proc) and (len(x) > 0)):\n",
    "            if len(chunk_ids)%2 == 0:\n",
    "                chunk_ids.append(x[-1])\n",
    "                x = x[:-1]\n",
    "            else:\n",
    "                chunk_ids.append(x[0])\n",
    "                x = x[1:]\n",
    "            chunk_size = sum(np.asarray(size_flist)[chunk_ids])\n",
    "        print(chunk_size)\n",
    "        chunks_ids.append(chunk_ids)\n",
    "    \n",
    "    if len(x) > 0:\n",
    "        warnings.warn(str(len(x)) + \" files have not been assigned to a processor.\")\n",
    "    \n",
    "    print('')\n",
    "    chunks=[]\n",
    "    skip_proc = 0\n",
    "    total_memory = 0\n",
    "    for j,chunk_ids in enumerate(chunks_ids):\n",
    "        print('Size in processor ' + str(j) + ' (MB):')\n",
    "        size_proc = sum(np.asarray(size_flist)[chunk_ids])\n",
    "        total_memory += size_proc\n",
    "        print(size_proc)\n",
    "        if size_proc == 0:\n",
    "            skip_proc += 1\n",
    "            continue\n",
    "        chunk = [flist[k] for k in chunk_ids]\n",
    "        chunks.append(chunk)\n",
    "\n",
    "    NPROC -= skip_proc\n",
    "        \n",
    "    print('')\n",
    "    print(\"Using \" + str(NPROC) + \" processors\")\n",
    "    \n",
    "    return NPROC, chunks, size_per_proc\n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "# Metadata\n",
    "parquet_filename = outdir_pqt + \"metadata/test_metadata.parquet\"\n",
    "df2.to_parquet(parquet_filename)\n",
    "print(str(parquet_filename) + \" stored.\")\n",
    "\n",
    "# Profiles\n",
    "print(\"Processing \" + str(len(flist)) + \" files.\")\n",
    "\n",
    "if not single_process:\n",
    "    nc_size_per_pqt = 40 # Empirically, 40 MB of average .nc file size gives in-memory sizes between 100-330 MB, which is what Dask recommens\n",
    "    NPROC, chunks,size_per_proc = poolParams(flist,nc_size_per_pqt)\n",
    "\n",
    "# fixing max nb of processes to prevent bottleneck likely due to I/O on disk queing operations and filling up the memory\n",
    "MAXPROC = 100\n",
    "if size_per_proc > 300:\n",
    "    MAXPROC = 12\n",
    "\n",
    "if NPROC > MAXPROC and not single_process:\n",
    "    print(\"Estimated number of processors might create bottleneck issues. Forcing to use \" + str(MAXPROC) + \" processors at a time.\")\n",
    "    # force to use at most MAXPROC processes, by looping over chunks\n",
    "    full_loops = NPROC//MAXPROC  #nb of loops to use at most MAXPROC\n",
    "    RESPROC = NPROC%MAXPROC   #nb of residual processors after the loops\n",
    "    pool_obj = multiprocessing.Pool(processes=MAXPROC)\n",
    "    i_start = 0\n",
    "    i_end   = 0\n",
    "    failed_files = []\n",
    "    for full_loop in range(full_loops):\n",
    "        i_start = MAXPROC*full_loop\n",
    "        i_end   = MAXPROC*(full_loop+1)\n",
    "        failed_files.append( pool_obj.starmap(xr2pqt, [(rank, chunk, full_loop) for rank, chunk in enumerate(chunks[i_start:i_end])] ) )\n",
    "    pool_obj.close()\n",
    "\n",
    "    # multiprocessing across residual processor pool with NPROC<MAXPROC\n",
    "    if RESPROC > 0:\n",
    "        pool_obj = multiprocessing.Pool(processes=RESPROC)\n",
    "        failed_files.append( pool_obj.starmap(xr2pqt, [(rank, chunk, full_loop+1) for rank, chunk in enumerate(chunks[(i_end+1):])] ) )\n",
    "        pool_obj.close()\n",
    "\n",
    "elif NPROC > 1 and not single_process:\n",
    "    failed_files = []\n",
    "    pool_obj = multiprocessing.Pool(processes=NPROC)\n",
    "    failed_files.append( pool_obj.starmap(xr2pqt, [(rank, chunk, 0) for rank, chunk in enumerate(chunks)] ) )\n",
    "    pool_obj.close()\n",
    "\n",
    "else:\n",
    "    failed_files = xr2pqt(0,flist,0)\n",
    "\n",
    "failed = []\n",
    "for f in failed_files:\n",
    "    for g in f:\n",
    "        if len(g) > 0:\n",
    "            for h in g:\n",
    "                failed.append(h)\n",
    "                print(h)\n",
    "\n",
    "print('Files that encountered an error and were not converted:')\n",
    "print(failed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting all metadata to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_filename = outdir_pqt + \"test_metadata.parquet\"\n",
    "df_list.to_parquet(parquet_filename)\n",
    "print(str(parquet_filename) + \" stored.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metadata2pqt(rank,files_list,refVARS):\n",
    "    rank_str = \"#\" + str(rank) + \": \"\n",
    "    \n",
    "    df_list = []\n",
    "    df_memory = 0\n",
    "    counter = 0\n",
    "    nb_files = len(files_list)\n",
    "    argo_file_fail = []\n",
    "    for argo_file in files_list:\n",
    "        counter += 1\n",
    "        if counter%10==0:\n",
    "            print(rank_str + \"processing file \" + str(counter) + \" of \" + str(nb_files))\n",
    "            \n",
    "        try:\n",
    "            ds = xr.load_dataset(argo_file, engine=\"argo\") #loading into memory the profile\n",
    "        except:\n",
    "            print(rank_str + 'Failed on ' + str(argo_file))\n",
    "            argo_file_fail.append(argo_file)\n",
    "        \n",
    "        d0 = ds[refVARS]\n",
    "\n",
    "        # adding dimension = PLATFORM_NUMBER and its value to the current array\n",
    "        d0=d0.assign_coords(PLATFORM_NUMBER=ds.PLATFORM_NUMBER.values[0])\n",
    "        for da in d0.data_vars:\n",
    "            d0[da]=d0[da].expand_dims(dim={\"PLATFORM_NUMBER\": 1}, axis=0)\n",
    "        \n",
    "        df = d0.to_dataframe()\n",
    "        df_memory += df.memory_usage().sum()/(10**6) # tracking memory usage (in GB)\n",
    "        df_list.append(df)\n",
    "\n",
    "        if df_memory > 200:\n",
    "            print(rank_str + \"In-memory filesize: \" + \"{:.2f}\".format(df_memory) + \" MB. This is above the recommended size for parquet.\")\n",
    "    \n",
    "    df_list = pd.concat(df_list)\n",
    "\n",
    "    print(rank_str + \"Returning list of dataframes to main processor.\")\n",
    "\n",
    "    return df_list, argo_file_fail\n",
    "\n",
    "####################################\n",
    "\n",
    "flist = glob.glob(\"/vortexfs1/share/boom/data/gdac_snapshot/202403-ArgoData/dac/*/*/*_Sprof.nc\")\n",
    "# refVARS = ['PARAMETER', 'SCIENTIFIC_CALIB_EQUATION', 'SCIENTIFIC_CALIB_COEFFICIENT', 'SCIENTIFIC_CALIB_COMMENT', 'SCIENTIFIC_CALIB_DATE']\n",
    "refVARS = [\"DATA_TYPE\",\"FORMAT_VERSION\",\"HANDBOOK_VERSION\",\"REFERENCE_DATE_TIME\",\"DATE_CREATION\",\"DATE_UPDATE\"]\n",
    "nb_of_checks = len(flist)\n",
    "\n",
    "NPROC = 100\n",
    "if NPROC > nb_of_checks:\n",
    "    NPROC = nb_of_checks\n",
    "CHUNK_SZ = int(np.ceil(nb_of_checks/NPROC))\n",
    "chunks = batched(flist,CHUNK_SZ)\n",
    "\n",
    "print(CHUNK_SZ)\n",
    "\n",
    "pool_obj = multiprocessing.Pool(processes=NPROC)\n",
    "outputs = pool_obj.starmap(metadata2pqt, [(rank, chunk, refVARS) for rank, chunk in enumerate(chunks)])\n",
    "pool_obj.close()\n",
    "\n",
    "df_list = []\n",
    "failed_files = []\n",
    "for processor_output in outputs:\n",
    "    df_list.append(processor_output[0])\n",
    "    if len(processor_output[1])>0:\n",
    "        for f in processor_output[1]:\n",
    "            failed_files.append(f)\n",
    "df_list = pd.concat(df_list)\n",
    "\n",
    "parquet_filename = outdir_pqt + \"test_metadata.parquet\"\n",
    "df_list.to_parquet(parquet_filename)\n",
    "print(str(parquet_filename) + \" stored.\")\n",
    "\n",
    "print(\"Failed files:\")\n",
    "print(failed_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading from parquet\n",
    "\n",
    "There are a couple of way to read parquet files. One is directly using pandas (make sure you have pyarrow, fastparquet or other suitable engine installed), the other is with Dask. Generally speaking, you'll want to use Dask if you need a large amount of data at the same time so that you can benefit from its parallelization. You should avoid Dask and just go for pandas whenever the data fits in your RAM.\n",
    "\n",
    "When reading parquet files with pandas, you can either specificy the file name if you know which file you want, or the directory containing all the parquet files. In latter case if you apply any filter, pandas and pyarrow will sort through all the files in the folder, reading into memory only the subsets that satisfy your filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BBP700',\n",
       " 'BBP700_ADJUSTED',\n",
       " 'BBP700_ADJUSTED_ERROR',\n",
       " 'BBP700_ADJUSTED_QC',\n",
       " 'BBP700_QC',\n",
       " 'BBP700_dPRES',\n",
       " 'CDOM',\n",
       " 'CDOM_ADJUSTED',\n",
       " 'CDOM_ADJUSTED_ERROR',\n",
       " 'CDOM_ADJUSTED_QC',\n",
       " 'CDOM_QC',\n",
       " 'CDOM_dPRES',\n",
       " 'CHLA',\n",
       " 'CHLA_ADJUSTED',\n",
       " 'CHLA_ADJUSTED_ERROR',\n",
       " 'CHLA_ADJUSTED_QC',\n",
       " 'CHLA_QC',\n",
       " 'CHLA_dPRES',\n",
       " 'CYCLE_NUMBER',\n",
       " 'DOXY',\n",
       " 'DOXY_ADJUSTED',\n",
       " 'DOXY_ADJUSTED_ERROR',\n",
       " 'DOXY_ADJUSTED_QC',\n",
       " 'DOXY_QC',\n",
       " 'DOXY_dPRES',\n",
       " 'JULD',\n",
       " 'LATITUDE',\n",
       " 'LONGITUDE',\n",
       " 'NITRATE',\n",
       " 'NITRATE_ADJUSTED',\n",
       " 'NITRATE_ADJUSTED_ERROR',\n",
       " 'NITRATE_ADJUSTED_QC',\n",
       " 'NITRATE_QC',\n",
       " 'NITRATE_dPRES',\n",
       " 'PH_IN_SITU_TOTAL',\n",
       " 'PH_IN_SITU_TOTAL_ADJUSTED',\n",
       " 'PH_IN_SITU_TOTAL_ADJUSTED_ERROR',\n",
       " 'PH_IN_SITU_TOTAL_ADJUSTED_QC',\n",
       " 'PH_IN_SITU_TOTAL_QC',\n",
       " 'PH_IN_SITU_TOTAL_dPRES',\n",
       " 'PLATFORM_NUMBER',\n",
       " 'PRES',\n",
       " 'PRES_ADJUSTED',\n",
       " 'PRES_ADJUSTED_ERROR',\n",
       " 'PRES_ADJUSTED_QC',\n",
       " 'PRES_QC',\n",
       " 'PSAL',\n",
       " 'PSAL_ADJUSTED',\n",
       " 'PSAL_ADJUSTED_ERROR',\n",
       " 'PSAL_ADJUSTED_QC',\n",
       " 'PSAL_QC',\n",
       " 'PSAL_dPRES',\n",
       " 'TEMP',\n",
       " 'TEMP_ADJUSTED',\n",
       " 'TEMP_ADJUSTED_ERROR',\n",
       " 'TEMP_ADJUSTED_QC',\n",
       " 'TEMP_QC',\n",
       " 'TEMP_dPRES']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel = [(\"PLATFORM_NUMBER\", \"==\", 6901494)]\n",
    "df = pd.read_parquet( glob.glob(outdir_pqt + \"test_profiles*\") , engine='pyarrow', filters = sel )\n",
    "sorted(df.columns.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"PSAL_ADJUSTED\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with metadata file\n",
    "sel = [(\"PLATFORM_NUMBER\", \"==\", 6990526)]\n",
    "df = pd.read_parquet( parquet_filename , engine='pyarrow', filters = sel )\n",
    "df[ [\"DATA_TYPE\", \"DATE_CREATION\"] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing conversion\n",
    "\n",
    "The following cell performs integrity tests on the parquet files. As the number of floats, profiles, and variables is large, the check is performed over all the variables, but not all the files. For each variable in `VARS`, files are randomly selected from the input list (in a number set to 5% of the .nc files) and for each of them, the selected `VARS` is compared to the one obtained from the parquet file. Each of these checks can:\n",
    "* succeed, if the nc and parquet variables are equal\n",
    "* fail, if the nc and parquet variables are not equal\n",
    "* be skipped, if the randomly selected file does not contain the selected variable\n",
    "\n",
    "If a file is skipped, another one is randomly selected, until a minimum number of files that contain the selected variable is found. For each variable, no file can be randomly picked two or more times (it can happen across variables).\n",
    "\n",
    "The variables `successes` and `fails` contain the file id and the name of the variable for which the check was succesful or failed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def checkVars(rank, flist, VARS):\n",
    "\n",
    "    rank_str = \"#\" + str(rank) + \": \"\n",
    "    \n",
    "    rand_max = len(flist)\n",
    "    nb_of_checks_per_var = np.min( [5,len(flist)] ) #int(np.ceil(rand_share*rand_max))\n",
    "    nb_of_checks = nb_of_checks_per_var*len(VARS)\n",
    "\n",
    "    print(rank_str + \"Checking \" + str(nb_of_checks) + \" random files.\")\n",
    "    \n",
    "    check_nb = 0\n",
    "    successes = []\n",
    "    fails = []\n",
    "    skipped = []\n",
    "    for v in range(len(VARS)):\n",
    "    \n",
    "        rand_idces = []\n",
    "        target_var = VARS[v]\n",
    "    \n",
    "        r = 0\n",
    "        while ((r < nb_of_checks_per_var) and (len(rand_idces) < len(flist) )):\n",
    "            print(rank_str + \"Check \" + str(r) + \" of \" + str(nb_of_checks_per_var) )\n",
    "            \n",
    "            check_nb += 1\n",
    "            rand_avail = np.arange(0,rand_max)[~np.isin(np.arange(0,rand_max), rand_idces)]\n",
    "            rand_idx = np.random.choice( rand_avail )\n",
    "            rand_idces.append(rand_idx)\n",
    "\n",
    "            try:\n",
    "                ref_ds = xr.load_dataset(flist[rand_idx], engine=\"argo\")\n",
    "            except:\n",
    "                print(rank_str + 'Failed on ' + str(flist[rand_idx]))\n",
    "                continue\n",
    "                \n",
    "            # print(rank_str + \"Reading file \" + flist[rand_idx] )\n",
    "            ref_platform = ref_ds.PLATFORM_NUMBER.values[0]\n",
    "        \n",
    "            invars = list(set(VARS) & set(list(ref_ds.data_vars)))\n",
    "            \n",
    "            if target_var not in invars:\n",
    "                ref_var = None\n",
    "                del ref_var\n",
    "                gc.collect()\n",
    "\n",
    "                skipped.append( (rand_idx, target_var ) )\n",
    "                # print(rank_str + \"Current random file does not contain variable \" + target_var + \", skipping this check.\")\n",
    "                continue\n",
    "                \n",
    "            print(rank_str + \"Checking \" + target_var + \" in platform number \" + str(ref_platform) + \".\")\n",
    "        \n",
    "            dim0 = ref_ds.sizes[\"N_PROF\"]\n",
    "            dim1 = ref_ds.sizes[\"N_LEVELS\"]\n",
    "        \n",
    "            if np.issubdtype(ref_ds[target_var].dtype, np.datetime64):\n",
    "                ref_var = np.zeros( dim0*dim1, dtype='datetime64[ns]' )\n",
    "            else:\n",
    "                ref_var = np.zeros( dim0*dim1, dtype=np.float64 )\n",
    "        \n",
    "            for j in range(dim0):\n",
    "                for k in range(dim1):\n",
    "                    ref_id = j*dim1+k\n",
    "                    if len(ref_ds[target_var].dims) > 1:\n",
    "                        ref_var[ref_id] = ref_ds[target_var][j,k].values\n",
    "                    else:\n",
    "                        ref_var[ref_id] = ref_ds[target_var][j].values\n",
    "        \n",
    "            sel_pqt = [(\"PLATFORM_NUMBER\", \"==\", ref_platform)]\n",
    "\n",
    "            try:\n",
    "                df_pqt = pd.read_parquet( outdir_pqt , engine='pyarrow', filters = sel_pqt )\n",
    "            except:\n",
    "                print(\"Loading parquet file failed for platform \" + str(ref_platform) + \"!\")\n",
    "                continue\n",
    "    \n",
    "            if target_var not in df_pqt.columns.tolist():\n",
    "                fails.append( (rand_idx, target_var ) )\n",
    "                r += 1\n",
    "                print(rank_str + \"Warning: \" + target_var + \" not found in parquet file.\")\n",
    "                continue\n",
    "        \n",
    "            df_pqt_var = df_pqt[target_var].values\n",
    "        \n",
    "            success = np.array_equal(ref_var, df_pqt_var, equal_nan=True)\n",
    "\n",
    "            ref_var = None\n",
    "            df_pqt_var = None\n",
    "            del ref_var\n",
    "            del df_pqt_var\n",
    "            gc.collect()\n",
    "            \n",
    "            if success:\n",
    "                successes.append( (flist[rand_idx], target_var ) )\n",
    "            else:\n",
    "                fails.append( (flist[rand_idx], target_var ) )\n",
    "    \n",
    "            r += 1\n",
    "\n",
    "    print(rank_str + \"All checks in process done\")\n",
    "    print(rank_str +  str(len(successes)) + \" checks were succesful.\")\n",
    "    print(rank_str +  str(len(fails)) + \" checks failed.\")\n",
    "\n",
    "    return successes, fails\n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "nb_of_checks = len(flist)\n",
    "\n",
    "NPROC = 20\n",
    "CHUNK_SZ = int(np.ceil(nb_of_checks/NPROC))\n",
    "chunks = batched(flist,CHUNK_SZ)\n",
    "\n",
    "print(CHUNK_SZ)\n",
    "\n",
    "# print(list(chunks))\n",
    "\n",
    "print(\"Checking \" + str(nb_of_checks) + \" random files.\")\n",
    "print(\"\")\n",
    "\n",
    "# print([(rank, chunk) for rank, chunk in enumerate(chunks)])\n",
    "pool_obj = multiprocessing.Pool(processes=NPROC)\n",
    "outputs = pool_obj.starmap(checkVars, [(rank, chunk, VARS) for rank, chunk in enumerate(chunks)])\n",
    "pool_obj.close()\n",
    "print(\"\")\n",
    "print(\"All checks were done.\")\n",
    "\n",
    "successes = []\n",
    "fails = []\n",
    "for output in outputs:\n",
    "    successes.append(output[0])\n",
    "    fails.append(output[1])\n",
    "\n",
    "print(\"Successful tests file names and variable name.\")\n",
    "print(successes)\n",
    "print(\"Failed tests file names and variable name.\")\n",
    "print(fails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing conversion\n",
    "\n",
    "The following cell checks that the parquet files contain all the floats, by checking that all original platform numbers exist. Note: it *does not* check that the variables of the original float exist and are correct in the parquet file (see previous cell for this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def checkPlatformNb(rank,flist):\n",
    "\n",
    "    rank_str = \"#\" + str(rank) + \": \"\n",
    "    outdir_pqt = '/vortexfs1/share/boom/data/nc2pqt_test/PQT/'\n",
    "    \n",
    "    check_nb = 0\n",
    "    successes = []\n",
    "    fails = []\n",
    "    failed_on_read = []\n",
    "\n",
    "    for idx in range(len(flist)):\n",
    "    \n",
    "        check_nb += 1\n",
    "        try:\n",
    "            ref_ds = xr.load_dataset(flist[idx], engine=\"argo\")\n",
    "        except:\n",
    "            print(rank_str + 'Failed on ' + str(flist[idx]))\n",
    "            failed_on_read.append(flist[idx])\n",
    "            continue\n",
    "        \n",
    "        ref_platform = ref_ds.PLATFORM_NUMBER.values[0]\n",
    "\n",
    "        ref_ds = None\n",
    "        del ref_ds\n",
    "        gc.collect()\n",
    "        \n",
    "        sel_pqt = [(\"PLATFORM_NUMBER\", \"==\", ref_platform)]\n",
    "        try:\n",
    "            df_pqt = pd.read_parquet( outdir_pqt , engine='pyarrow', filters = sel_pqt )\n",
    "        except:\n",
    "            fails.append( (idx ) )\n",
    "            continue\n",
    "\n",
    "        df_pqt = None\n",
    "        del df_pqt\n",
    "        gc.collect()\n",
    "        successes.append( (idx ) )\n",
    "        \n",
    "        if check_nb%10:\n",
    "        print(rank_str + \"Check \" + str(check_nb) + \" of \" + str(len(flist)) + \" done.\")\n",
    "\n",
    "    print(rank_str + \"All checks in process done\")\n",
    "    print(rank_str +  str(len(successes)) + \" checks were succesful.\")\n",
    "    print(rank_str +  str(len(fails)) + \" checks failed.\")\n",
    "    if len(failed_on_read)>0:\n",
    "        print(rank_str +  str(len(failed_on_read)) + \" original Argo file(s) could not be loaded, likely due to errors in the original file. These files were likely never converted to parquet.\")\n",
    "        print(\"File list:\")\n",
    "        print(failed_on_read)\n",
    "    else:\n",
    "        print(rank_str +  str(len(failed_on_read)) + \" original Argo file(s) could not be loaded.\")\n",
    "\n",
    "############################################################################################################\n",
    "\n",
    "flist = glob.glob(\"/vortexfs1/share/boom/data/gdac_snapshot/202403-ArgoData/dac/coriolis/*/*_Sprof.nc\")\n",
    "\n",
    "nb_of_checks = len(flist)\n",
    "\n",
    "NPROC = 20\n",
    "CHUNK_SZ = int(np.ceil(nb_of_checks/NPROC))\n",
    "chunks = batched(flist,CHUNK_SZ)\n",
    "\n",
    "print(CHUNK_SZ)\n",
    "\n",
    "# print(list(chunks))\n",
    "\n",
    "print(\"Checking \" + str(nb_of_checks) + \" random files.\")\n",
    "print(\"\")\n",
    "\n",
    "pool_obj = multiprocessing.Pool(processes=NPROC)\n",
    "pool_obj.starmap(checkPlatformNb, [(rank, chunk) for rank, chunk in enumerate(chunks)])\n",
    "pool_obj.close()\n",
    "\n",
    "print(\"\")\n",
    "print(\"All checks were done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Example loading Sprof from snapshot\n",
    "```\n",
    "ds = xr.load_dataset('/vortexfs1/share/boom/data/gdac_snapshot/202403-ArgoData/dac/aoml/1902304/1902304_Sprof.nc')\n",
    "df = ds.to_dataframe()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import os\n",
    "from pyarrow import fs\n",
    "s3 = fs.S3FileSystem(region='us-east-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from pyarrow import Table\n",
    "\n",
    "ds = xr.load_dataset('/vortexfs1/share/boom/data/gdac_snapshot/202403-ArgoData/dac/aoml/1902304/1902304_Sprof.nc',engine=\"argo\")\n",
    "df = ds[['DOXY','PRES','NITRATE','PLATFORM_NUMBER']].to_dataframe()\n",
    "\n",
    "s3_filepath = 'data.parquet'\n",
    "\n",
    "pq.write_to_dataset(\n",
    "    Table.from_pandas(df),\n",
    "    s3_filepath,\n",
    "    filesystem=s3,\n",
    "    use_dictionary=True,\n",
    "    compression=\"snappy\",\n",
    "    version=\"2.4\",\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
